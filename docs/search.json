[
  {
    "objectID": "class_notes/week_7.html",
    "href": "class_notes/week_7.html",
    "title": "Week 7: Classifying",
    "section": "",
    "text": "Look at running a principal components analysis for authorship\nWork on some models for classifying data\nDiscuss how we might evaluate our models\n\nThis week, we will start by moving from clustering to PCA, or pricinpal components analysis. PCA is often seen as a way of reducing the dimensions of features, and here we will explore what it looks like when exploring questions of authorship.\nWe will follow this by looking at what it means to train a classifier, and some of the research questions we might ask of a classifier. ## Getting Started We will be using a of libraries today:\n\n\nPCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.\nHere we load the data in:\n\n# complete_data &lt;- read_csv(here(\"data\", \"attribution_data_new.csv\", na.strings=c(\"\",\"NA\", header=T)))\ncomplete_data &lt;- read.csv(\"../data/attribution_data_new.csv\", na.strings=c(\"\",\"NA\"), header=T)\ncomplete_data &lt;- complete_data[,-62]\n\nJesse Rodin’s Josquin Research Project has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.\n\n# Josquin attribution level 1 and palestrina\n\njosquin &lt;- complete_data[complete_data$Composer == 'Josquin des Prez',-12]\n\njosquin_secure &lt;- josquin[josquin$Attribution.Level &lt;= 2 ,]\njosquin_secure$Composer &lt;- as.character(josquin_secure$Composer)\njosquin_less_secure &lt;- josquin[ josquin$Attribution.Level &gt;= 3,]\n\n\n####Other composers\nbach &lt;- complete_data[complete_data$Composer == \"Bach_Johann Sebastian\",-12]\nlarue &lt;- complete_data[complete_data$Composer == \"la Rue_Pierre de\",-12]\npalestrina &lt;- complete_data[complete_data$Composer == \"Palestrina_Giovanni Perluigi da\",-12]\nockeghem &lt;- complete_data[complete_data$Composer == \"Johannes Ockeghem\",-12]\norto &lt;- complete_data[complete_data$Composer == \"de Orto_Marbrianus\",-12]\ndufay &lt;- complete_data[complete_data$Composer == \"Du Fay_Guillaume\",-12]\n\njosquin_bach &lt;- rbind(josquin_secure, bach)\njosquin_palestrina &lt;- rbind(josquin_secure, palestrina)\njosquin_larue &lt;- rbind(josquin_secure, larue)\n\ncomparison &lt;- rbind(josquin_secure, dufay)\n\n\ncolumns_wanted &lt;- c(5:11)  \nMatrix &lt;- comparison[,columns_wanted]\nMatrix &lt;- as.matrix(Matrix)\nMatrix[is.na(Matrix)] &lt;- 0\n# log.pieces &lt;- log(Matrix)\nlog.pieces &lt;- log(Matrix)\ncomposer &lt;- comparison[,1]\n\nThis code runs the actual principal components analysis.\nIt also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.\n\n####principle component analysis.\n\npieces.pca &lt;- prcomp(Matrix,\n                 center = TRUE,\n                 scale. = TRUE) \nplot(pieces.pca, type = \"l\", main=\"Principal Components Analysis\")\n\n\n\n\nIt’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.\n\nprint(pieces.pca)\n\nStandard deviations (1, .., p=7):\n[1] 1.3651251 1.1932956 1.0473249 0.9758057 0.8158066 0.7627338 0.6450502\n\nRotation (n x k) = (7 x 7):\n                         PC1         PC2         PC3         PC4         PC5\nnPVI_Entire       -0.1534310  0.28077115 -0.77204065  0.00852347  0.47773321\nNine_Eight        -0.1018707  0.59859586  0.02341681  0.52670532 -0.12881933\npitch_correlation -0.1550940  0.39505005  0.02896214 -0.83452001 -0.15900704\npitch_entropy     -0.1600989  0.50110624  0.56438102  0.04875023  0.32761392\nparallel_motion    0.4600560  0.38613864 -0.26230330 -0.01533229 -0.49612767\nsimilar_motion     0.6300842  0.05699415  0.06600453  0.03642203 -0.03731014\noblique_motion    -0.5547412 -0.05768156 -0.10430981  0.14881862 -0.61239508\n                         PC6         PC7\nnPVI_Entire       -0.2145105 -0.16511611\nNine_Eight         0.5736302 -0.08770651\npitch_correlation  0.2642076 -0.16592149\npitch_entropy     -0.5428772  0.01765806\nparallel_motion   -0.3378146  0.45819860\nsimilar_motion    -0.1086246 -0.76214894\noblique_motion    -0.3667346 -0.38260361\n\n\nAs we can see, about 65% of the variance is accounted for with the first two principal components:\n\nsummary(pieces.pca)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.3651 1.1933 1.0473 0.9758 0.81581 0.76273 0.64505\nProportion of Variance 0.2662 0.2034 0.1567 0.1360 0.09508 0.08311 0.05944\nCumulative Proportion  0.2662 0.4697 0.6263 0.7624 0.85745 0.94056 1.00000\n\n\nPlotting our two composers with the first two principal components.\n\ng &lt;- ggbiplot(pieces.pca, obs.scale = 1, var.scale = 1, \n              groups = composer, ellipse = TRUE, \n              circle = TRUE)\ng &lt;- g + scale_color_discrete(name = '')\ng &lt;- g + theme(legend.direction = 'horizontal', \n               legend.position = 'top') +\n               theme_bw()\nprint(g)\n\n\n\n# we can change the number of components\n# seven_component_model &lt;- data.frame(pieces.pca$x[,1:8])\n\nWe can also look at how much each of these features is being weighted within the first two components.\n\ntheta &lt;- seq(0,2*pi,length.out = 100)\ncircle &lt;- data.frame(x = cos(theta), y = sin(theta))\np &lt;- ggplot(circle,aes(x,y)) + geom_path()\n\nloadings &lt;- data.frame(pieces.pca$rotation, \n                       .names = row.names(pieces.pca$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n  coord_fixed(ratio=1) +\n  labs(x = \"PC1\", y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\nA classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week. Our goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week.\n\n\n\nFirst, let’s get the data and add a column that tells us whether it’s a Christmas song or not\n\n### get the data and add yes/no column.\nchristmas &lt;- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas &lt;- \"yes\"\n\nnot &lt;- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas &lt;- \"no\"\n\n## combine the two datasets and get the columns we want to use.\nchristmas_subset &lt;-rbind(christmas, not)\nchristmas_subset &lt;- christmas_subset |&gt; \n    select(c(\"christmas\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nNow we can use the createDataPartition function from the caret library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.\n\nTrain &lt;- createDataPartition(christmas_subset$christmas, p=0.7, list=FALSE)\ntraining &lt;- christmas_subset[ Train, ]\ntesting &lt;- christmas_subset[ -Train, ]\n\nWe can pretty easily implement something like a neural network, using our training dataset to train it:\n\nmod_fit &lt;- caret::train(christmas ~ .,  \n                 data=training, method=\"nnet\", importance = \"christmas\")\n\nOnce we’ve trained this model, we can test it on our testing dataset, and see how well it does:\n\npred &lt;- predict(mod_fit, testing)\nconfusionMatrix(pred, as.factor(testing$christmas), positive = \"yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction no yes\n       no  29   9\n       yes  6  21\n                                          \n               Accuracy : 0.7692          \n                 95% CI : (0.6481, 0.8647)\n    No Information Rate : 0.5385          \n    P-Value [Acc &gt; NIR] : 0.0001027       \n                                          \n                  Kappa : 0.5324          \n                                          \n Mcnemar's Test P-Value : 0.6055766       \n                                          \n            Sensitivity : 0.7000          \n            Specificity : 0.8286          \n         Pos Pred Value : 0.7778          \n         Neg Pred Value : 0.7632          \n             Prevalence : 0.4615          \n         Detection Rate : 0.3231          \n   Detection Prevalence : 0.4154          \n      Balanced Accuracy : 0.7643          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\nSo what does this all mean? Let’s define some terms.\n\nAccuracy:\n\nthe accuracy rate. Just how many things it got right.\n\n95% CI:\n\nthe confidence interval of the accuracy.\n\nNo information rate:\n\ngiven no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”\nif you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.\n\nP-Value:\n\nlikelihood of chance.\n\nKappa:\n\nmeasures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.\n\nMcNemar’s Test P-Value:\n\nthis is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,\n\nSensitivity:\n\ngiven that a result is actually a thing, what is the probability that our model will predict that event’s results?\n\nSpecificity:\n\ngiven that a result is not actually a thing, what is the probability that our model will predict that?\n\nPos Predictive Value:\n\nthe probability that a predicted ‘positive’ class is actually positive.\n\nNeg Predictive Value:\n\nthe probability that a predicted ‘negative’ class is actually negative.\n\nPrevalence:\n\nthe prevalence of the ‘positive event’\n\nDetection Rate:\n\nthe rate of true events also predicted to be events\n\nDetection Prevalence\n\nthe prevalence of predicted events\n\nBalanced Accuracy:\n\nthe average of the proportion corrects of each class individually\n\n\n\n\nWe can look at which features the model is using…\n\nplot(varImp(mod_fit))\n\n\n\n\n\n\n\n\n\nUse PCA to explore the works of two artists. How well do they “separate”?\nRun a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?"
  },
  {
    "objectID": "class_notes/week_7.html#pca-and-authorship",
    "href": "class_notes/week_7.html#pca-and-authorship",
    "title": "Week 7: Classifying",
    "section": "",
    "text": "PCAs are often used for reducing dimensions when we have lots of variables but a model might be better suited from combining those variables. PCAs have also been used a fair bit to explore questions of authorship. Here we have a question of authorship using symbolic data taken from scores. We are trying to explore the music of Josquin.\nHere we load the data in:\n\n# complete_data &lt;- read_csv(here(\"data\", \"attribution_data_new.csv\", na.strings=c(\"\",\"NA\", header=T)))\ncomplete_data &lt;- read.csv(\"../data/attribution_data_new.csv\", na.strings=c(\"\",\"NA\"), header=T)\ncomplete_data &lt;- complete_data[,-62]\n\nJesse Rodin’s Josquin Research Project has given levels of security for attribution, including pieces that we know are Josquin’s, those we think might be, and those which are more questionable.\n\n# Josquin attribution level 1 and palestrina\n\njosquin &lt;- complete_data[complete_data$Composer == 'Josquin des Prez',-12]\n\njosquin_secure &lt;- josquin[josquin$Attribution.Level &lt;= 2 ,]\njosquin_secure$Composer &lt;- as.character(josquin_secure$Composer)\njosquin_less_secure &lt;- josquin[ josquin$Attribution.Level &gt;= 3,]\n\n\n####Other composers\nbach &lt;- complete_data[complete_data$Composer == \"Bach_Johann Sebastian\",-12]\nlarue &lt;- complete_data[complete_data$Composer == \"la Rue_Pierre de\",-12]\npalestrina &lt;- complete_data[complete_data$Composer == \"Palestrina_Giovanni Perluigi da\",-12]\nockeghem &lt;- complete_data[complete_data$Composer == \"Johannes Ockeghem\",-12]\norto &lt;- complete_data[complete_data$Composer == \"de Orto_Marbrianus\",-12]\ndufay &lt;- complete_data[complete_data$Composer == \"Du Fay_Guillaume\",-12]\n\njosquin_bach &lt;- rbind(josquin_secure, bach)\njosquin_palestrina &lt;- rbind(josquin_secure, palestrina)\njosquin_larue &lt;- rbind(josquin_secure, larue)\n\ncomparison &lt;- rbind(josquin_secure, dufay)\n\n\ncolumns_wanted &lt;- c(5:11)  \nMatrix &lt;- comparison[,columns_wanted]\nMatrix &lt;- as.matrix(Matrix)\nMatrix[is.na(Matrix)] &lt;- 0\n# log.pieces &lt;- log(Matrix)\nlog.pieces &lt;- log(Matrix)\ncomposer &lt;- comparison[,1]\n\nThis code runs the actual principal components analysis.\nIt also provides a scree plot, allowing us to see which components are the most heavily weighted. This can allow us to reduce the dimensions as we see fit.\n\n####principle component analysis.\n\npieces.pca &lt;- prcomp(Matrix,\n                 center = TRUE,\n                 scale. = TRUE) \nplot(pieces.pca, type = \"l\", main=\"Principal Components Analysis\")\n\n\n\n\nIt’s worth taking some time to explore what each of these components actually means and how they’re weighted. PCA is weighting instances of parallel motion and similar motion pretty heavily, but negatively weighting pitch entropy and oblique motion. PC2 seems to be looking at nPVI and 9-8 suspensions.\n\nprint(pieces.pca)\n\nStandard deviations (1, .., p=7):\n[1] 1.3651251 1.1932956 1.0473249 0.9758057 0.8158066 0.7627338 0.6450502\n\nRotation (n x k) = (7 x 7):\n                         PC1         PC2         PC3         PC4         PC5\nnPVI_Entire       -0.1534310  0.28077115 -0.77204065  0.00852347  0.47773321\nNine_Eight        -0.1018707  0.59859586  0.02341681  0.52670532 -0.12881933\npitch_correlation -0.1550940  0.39505005  0.02896214 -0.83452001 -0.15900704\npitch_entropy     -0.1600989  0.50110624  0.56438102  0.04875023  0.32761392\nparallel_motion    0.4600560  0.38613864 -0.26230330 -0.01533229 -0.49612767\nsimilar_motion     0.6300842  0.05699415  0.06600453  0.03642203 -0.03731014\noblique_motion    -0.5547412 -0.05768156 -0.10430981  0.14881862 -0.61239508\n                         PC6         PC7\nnPVI_Entire       -0.2145105 -0.16511611\nNine_Eight         0.5736302 -0.08770651\npitch_correlation  0.2642076 -0.16592149\npitch_entropy     -0.5428772  0.01765806\nparallel_motion   -0.3378146  0.45819860\nsimilar_motion    -0.1086246 -0.76214894\noblique_motion    -0.3667346 -0.38260361\n\n\nAs we can see, about 65% of the variance is accounted for with the first two principal components:\n\nsummary(pieces.pca)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.3651 1.1933 1.0473 0.9758 0.81581 0.76273 0.64505\nProportion of Variance 0.2662 0.2034 0.1567 0.1360 0.09508 0.08311 0.05944\nCumulative Proportion  0.2662 0.4697 0.6263 0.7624 0.85745 0.94056 1.00000\n\n\nPlotting our two composers with the first two principal components.\n\ng &lt;- ggbiplot(pieces.pca, obs.scale = 1, var.scale = 1, \n              groups = composer, ellipse = TRUE, \n              circle = TRUE)\ng &lt;- g + scale_color_discrete(name = '')\ng &lt;- g + theme(legend.direction = 'horizontal', \n               legend.position = 'top') +\n               theme_bw()\nprint(g)\n\n\n\n# we can change the number of components\n# seven_component_model &lt;- data.frame(pieces.pca$x[,1:8])\n\nWe can also look at how much each of these features is being weighted within the first two components.\n\ntheta &lt;- seq(0,2*pi,length.out = 100)\ncircle &lt;- data.frame(x = cos(theta), y = sin(theta))\np &lt;- ggplot(circle,aes(x,y)) + geom_path()\n\nloadings &lt;- data.frame(pieces.pca$rotation, \n                       .names = row.names(pieces.pca$rotation))\np + geom_text(data=loadings, \n              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +\n  coord_fixed(ratio=1) +\n  labs(x = \"PC1\", y = \"PC2\") +\n  theme_bw()"
  },
  {
    "objectID": "class_notes/week_7.html#classifiers",
    "href": "class_notes/week_7.html#classifiers",
    "title": "Week 7: Classifying",
    "section": "",
    "text": "A classifier is a model that assigns a label to data based on the input. There are many types of classifiers, and we will be evaluating various models throughout the week. Our goal will be to train a model on the features generally associated with a category, and then test the accuracy of that model. For now, a good starting point might be our Christmas Song question from last week."
  },
  {
    "objectID": "class_notes/week_7.html#returning-to-our-christmas-song-problem",
    "href": "class_notes/week_7.html#returning-to-our-christmas-song-problem",
    "title": "Week 7: Classifying",
    "section": "",
    "text": "First, let’s get the data and add a column that tells us whether it’s a Christmas song or not\n\n### get the data and add yes/no column.\nchristmas &lt;- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas &lt;- \"yes\"\n\nnot &lt;- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas &lt;- \"no\"\n\n## combine the two datasets and get the columns we want to use.\nchristmas_subset &lt;-rbind(christmas, not)\nchristmas_subset &lt;- christmas_subset |&gt; \n    select(c(\"christmas\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nNow we can use the createDataPartition function from the caret library to create a testing and a training dataset. Here, I’ve chosen a 70/30 partition of training and testing, but you can adjust as you see fit.\n\nTrain &lt;- createDataPartition(christmas_subset$christmas, p=0.7, list=FALSE)\ntraining &lt;- christmas_subset[ Train, ]\ntesting &lt;- christmas_subset[ -Train, ]\n\nWe can pretty easily implement something like a neural network, using our training dataset to train it:\n\nmod_fit &lt;- caret::train(christmas ~ .,  \n                 data=training, method=\"nnet\", importance = \"christmas\")\n\nOnce we’ve trained this model, we can test it on our testing dataset, and see how well it does:\n\npred &lt;- predict(mod_fit, testing)\nconfusionMatrix(pred, as.factor(testing$christmas), positive = \"yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction no yes\n       no  29   9\n       yes  6  21\n                                          \n               Accuracy : 0.7692          \n                 95% CI : (0.6481, 0.8647)\n    No Information Rate : 0.5385          \n    P-Value [Acc &gt; NIR] : 0.0001027       \n                                          \n                  Kappa : 0.5324          \n                                          \n Mcnemar's Test P-Value : 0.6055766       \n                                          \n            Sensitivity : 0.7000          \n            Specificity : 0.8286          \n         Pos Pred Value : 0.7778          \n         Neg Pred Value : 0.7632          \n             Prevalence : 0.4615          \n         Detection Rate : 0.3231          \n   Detection Prevalence : 0.4154          \n      Balanced Accuracy : 0.7643          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\nSo what does this all mean? Let’s define some terms.\n\nAccuracy:\n\nthe accuracy rate. Just how many things it got right.\n\n95% CI:\n\nthe confidence interval of the accuracy.\n\nNo information rate:\n\ngiven no more information other than the overall distribution, how likely are you to be correct if you just pick the “majority class.”\nif you have an accuracy rate of 80%, but the majority class is 80%, then your model isn’t terribly useful.\n\nP-Value:\n\nlikelihood of chance.\n\nKappa:\n\nmeasures the agreement between two raters and ratings. Here it’s looking at the difference between observed accuracy and random chance given the distribution in the dataset.\n\nMcNemar’s Test P-Value:\n\nthis is looking at the two distributions (from a 2x2 table), and determines if they are significantly different,\n\nSensitivity:\n\ngiven that a result is actually a thing, what is the probability that our model will predict that event’s results?\n\nSpecificity:\n\ngiven that a result is not actually a thing, what is the probability that our model will predict that?\n\nPos Predictive Value:\n\nthe probability that a predicted ‘positive’ class is actually positive.\n\nNeg Predictive Value:\n\nthe probability that a predicted ‘negative’ class is actually negative.\n\nPrevalence:\n\nthe prevalence of the ‘positive event’\n\nDetection Rate:\n\nthe rate of true events also predicted to be events\n\nDetection Prevalence\n\nthe prevalence of predicted events\n\nBalanced Accuracy:\n\nthe average of the proportion corrects of each class individually\n\n\n\n\nWe can look at which features the model is using…\n\nplot(varImp(mod_fit))"
  },
  {
    "objectID": "class_notes/week_7.html#exercise",
    "href": "class_notes/week_7.html#exercise",
    "title": "Week 7: Classifying",
    "section": "",
    "text": "Use PCA to explore the works of two artists. How well do they “separate”?\nRun a classifier on two groups (it can be the same two artists, or two distinct groups). How well does your model do?"
  },
  {
    "objectID": "class_notes/week_7.html#john-or-paul",
    "href": "class_notes/week_7.html#john-or-paul",
    "title": "Week 7: Classifying",
    "section": "John or Paul?",
    "text": "John or Paul?\nOur research process will follow a simple trajectory:\n\nGet songs by each artist’s solo career (this can be our ‘ground truth’, as it were).\nTrain the model on these pieces, and evaluate the various models.\nApply the various models to some songs by the Beatles.\n\n\nGetting the Data\n\njohn &lt;- get_artist_audio_features('john lennon')\npaul &lt;- get_artist_audio_features('paul mccartney')\nboth &lt;- rbind(john, paul)\n\nWhat is the balance of pieces like? It looks like we have far more McCartney than Lennon pieces. What does this mean for our model?\n\ntable(both$artist_name)\n\n\n   John Lennon Paul McCartney \n           584            984 \n\n\nWe then can grab only the features that we want to explore for this model.\n\nboth_subset &lt;- both |&gt; select(c(\"artist_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nBefore running a clustering, PCA, or a classifier such as a k-nearest neighbor, it’s probably good to standardize your data. This means that the data is consistent, and prevents wide ranges from dominating the results. Here we’ve scaled all of our data with the z-score of the data according with the rest of the data for that category.\nI’ve also (temporarily) split the data from the artist, and then brought it all back together with cbind.\n\ndata &lt;- both_subset[,-1]\nartists &lt;- both_subset[,1]\ndata &lt;- data %&gt;% mutate_all(~(scale(.) %&gt;% as.vector))\nboth_artists &lt;- cbind(artists, data)"
  },
  {
    "objectID": "class_notes/week_7.html#cross-validation",
    "href": "class_notes/week_7.html#cross-validation",
    "title": "Week 7: Classifying",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nCross-validation splits the data up into a testing and training set, and evaluates it.\n\nK-folds cross validation:\nK refers to the number of groups that data is split into.\n\nIt randomizes the data\nsplits it into the specified number of groups\nfor each group, split into a training and testing set, and then evaluate\n\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 2, savePredictions = TRUE)\n\n\nTrain &lt;- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\ntraining &lt;- both_artists[ Train, ]\ntesting &lt;- both_artists[ -Train, ]\n\nLet’s look at our results with a logistic regression:\n\nmod_fit &lt;- train(artists ~ .,  data=both_artists, method=\"glm\", family=\"binomial\",\n                 trControl = ctrl, tuneLength = 10)\n\ntesting$artists &lt;- as.factor(testing$artists)\npred &lt;- predict(mod_fit, newdata=testing)\nconfusionMatrix(data=pred, testing$artists)\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon             73             54\n  Paul McCartney         102            241\n                                          \n               Accuracy : 0.6681          \n                 95% CI : (0.6235, 0.7105)\n    No Information Rate : 0.6277          \n    P-Value [Acc &gt; NIR] : 0.0379757       \n                                          \n                  Kappa : 0.2479          \n                                          \n Mcnemar's Test P-Value : 0.0001679       \n                                          \n            Sensitivity : 0.4171          \n            Specificity : 0.8169          \n         Pos Pred Value : 0.5748          \n         Neg Pred Value : 0.7026          \n             Prevalence : 0.3723          \n         Detection Rate : 0.1553          \n   Detection Prevalence : 0.2702          \n      Balanced Accuracy : 0.6170          \n                                          \n       'Positive' Class : John Lennon     \n                                          \n\n\nIt looks like the accuracy is about 76%, but pay attention to the sensitivity and the specificity values.\nRecall that sensitivity is a measurement of how well the model can detect a “positive” instance, and specificity measures how well the model is finding true negatives.\nSensitivity can be defined as follows:\n\nSensitivity = (True Positive)/(True Positive + False Negative)\n\nand specificity can be defined as follows:\n\nSpecificity = (True Negative)/(True Negative + False Positive)\n\nSo this model is quite good at finding the negative class (here defined as McCartney), but not great at finding the positive class (Lennon)."
  },
  {
    "objectID": "class_notes/week_7.html#other-models",
    "href": "class_notes/week_7.html#other-models",
    "title": "Week 7: Classifying",
    "section": "Other Models",
    "text": "Other Models\nLet’s run the same code again, but now with a k-nearest neighbor. For our sanity, let’s put it into a function.\n\nmodel_evaluation &lt;- function(method){\n    Train &lt;- createDataPartition(both_artists$artists, p=0.7, list=FALSE)\n    training &lt;- both_artists[ Train, ]\n    testing &lt;- both_artists[ -Train, ]\n    mod_fit &lt;- train(artists ~ .,  \n                     data=training, method=method)\n    pred &lt;- predict(mod_fit, newdata=testing)\n\n    accuracy &lt;- table(pred, testing[,\"artists\"])\n    sum(diag(accuracy))/sum(accuracy)\n    testing$artists &lt;- as.factor(testing$artists)\n    confusionMatrix(data=pred, testing$artists)\n    \n}\nmodel_evaluation(\"kknn\")\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       John Lennon Paul McCartney\n  John Lennon            119             48\n  Paul McCartney          56            247\n                                          \n               Accuracy : 0.7787          \n                 95% CI : (0.7384, 0.8155)\n    No Information Rate : 0.6277          \n    P-Value [Acc &gt; NIR] : 1.411e-12       \n                                          \n                  Kappa : 0.5221          \n                                          \n Mcnemar's Test P-Value : 0.4925          \n                                          \n            Sensitivity : 0.6800          \n            Specificity : 0.8373          \n         Pos Pred Value : 0.7126          \n         Neg Pred Value : 0.8152          \n             Prevalence : 0.3723          \n         Detection Rate : 0.2532          \n   Detection Prevalence : 0.3553          \n      Balanced Accuracy : 0.7586          \n                                          \n       'Positive' Class : John Lennon     \n                                          \n\n\nNote that it performs quite well! It’s better at finding the “John Lennon” model.\nWhy do we think this model performed better? A comparison of models can be found here.\n\nNeural Net\nA neural net doesn’t seem to do as well.\n\nmodel_evaluation(\"nnet\")"
  },
  {
    "objectID": "class_notes/week_7.html#comparing-models",
    "href": "class_notes/week_7.html#comparing-models",
    "title": "Week 7: Classifying",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nLogistic Regression\nK-nearest neighbor\nneural net\nLearning Vector Quantization\ngradient boosted machine\nsupport vector machine\n\nWe can train different models explicitly (without a function) for now.\n\nset.seed(1234)\ncontrol &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n# train logistic regression\nmodelglm &lt;- train(artists ~ ., data=both_artists, method=\"glm\", trControl=control)\n\n# train knn\nmodelknn &lt;- train(artists ~ ., data=both_artists, method=\"kknn\", trControl=control)\n\n# train nnet\nmodelnnet &lt;- train(artists ~ ., data=both_artists, method=\"nnet\", trControl=control)\n\n# train the LVQ model\nmodelLvq &lt;- train(artists ~ ., data=both_artists, method=\"lvq\", trControl=control)\n\n# train the GBM model\nset.seed(7)\nmodelGbm &lt;- train(artists ~ ., data=both_artists, method=\"gbm\", trControl=control)\n\n# train the SVM model\nset.seed(7)\nmodelSvm &lt;- train(artists ~., data=both_artists, method=\"svmRadial\", trControl=control)\n\n# train the random forest\nrandomforest &lt;- train(artists~., data=both_artists, method=\"ranger\", trControl=control)\n\nWe can actually look at the resampling of the dataset for each model, and get the results for each model:\n\n# collect resamples\nresults &lt;- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm,knn=modelknn, nnet=modelnnet, glm=modelglm, rf=randomforest))\n\n# summarize the distributions\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LVQ, GBM, SVM, knn, nnet, glm, rf \nNumber of resamples: 30 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  0.6050955 0.6415973 0.6602564 0.6600220 0.6751592 0.7133758    0\nGBM  0.6962025 0.7439980 0.7619124 0.7663809 0.7878695 0.8397436    0\nSVM  0.6687898 0.7179487 0.7388535 0.7387526 0.7687306 0.8012821    0\nknn  0.7324841 0.7647041 0.7809603 0.7832078 0.8012821 0.8333333    0\nnnet 0.6794872 0.7208987 0.7324841 0.7363845 0.7527715 0.8076923    0\nglm  0.6089744 0.6634820 0.6858974 0.6862284 0.7099359 0.7594937    0\nrf   0.7388535 0.8028602 0.8210844 0.8237734 0.8450584 0.9050633    0\n\nKappa \n           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLVQ  0.08665105 0.1683163 0.2101309 0.2206380 0.2633926 0.3847244    0\nGBM  0.32297804 0.4362291 0.4827239 0.4877173 0.5394920 0.6557203    0\nSVM  0.27879859 0.3771507 0.4167157 0.4276538 0.4885911 0.5730932    0\nknn  0.42590980 0.4981390 0.5401244 0.5399265 0.5824790 0.6505858    0\nnnet 0.29398986 0.3882580 0.4259220 0.4245391 0.4615117 0.5824411    0\nglm  0.07503888 0.2320983 0.2821086 0.2882337 0.3352471 0.4441770    0\nrf   0.44522968 0.5735189 0.6111771 0.6168420 0.6640556 0.7935900    0\n\n\nIt might be better to look at the accuracy for each model. Here we have the accuracy rating as well as Cohen’s Kappa, which is like accuracy but also incorporates the imbalance of the dataset.\n\n# boxplots of results\nbwplot(results)\n\n\n\n\nHere’s another plot:\n\n# dot plots of results\ndotplot(results)\n\n\n\n\nIs it possible to use this for a research question??\nWhat if we use our neural net model but on a different dataset? How about the beatles dataset that is available on Spotify (which admittedly isn’t as much as we’d like).\n\nGrabbing Beatles Data\nWe can start by getting the data from Spotify:\n\nbeatles &lt;- get_artist_audio_features('the beatles')\nbeatles_subset &lt;- beatles |&gt; select(c(\"artist_name\", \"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\n\nPredicting\nNow we can use the models that we’ve trained, but on new data. Here we use the k-nearest neighbor models.\n\nbeatles_knn &lt;- predict(modelknn, newdata=beatles_subset)\nbeatles_nnet &lt;- predict(modelnnet, newdata=beatles_subset)\nbeatles_glm &lt;- predict(modelglm, newdata=beatles_subset)\nbeatles_svm &lt;- predict(modelSvm, newdata=beatles_subset) \n\nNow, we are going to create a data frame of the track name, and the results from the model.\n\nclassified_data &lt;- as.data.frame(cbind(beatles_knn, beatles_nnet, beatles_glm, beatles_svm, beatles$track_name))\n\n(Note that this table doesn’t seem to be rendering correctly when pushed online)."
  },
  {
    "objectID": "class_notes/week_7.html#summary",
    "href": "class_notes/week_7.html#summary",
    "title": "Week 7: Classifying",
    "section": "Summary",
    "text": "Summary\nWhat I like about this is that we can take something about authorship that we know, and then use it to explore authorship of things that are a little more ambiguous. It can also teach us a fair bit about the specific models. Why do we think some performed so much better than others?\n\nExercise:\nLet’s try to build an east/west coast rap classifier:\nSteps!:\n\nGrab data\npartition and train model\ncompare models\nuse it to predict a new dataset.\n\n\neast_coast &lt;- get_playlist_audio_features(\"\", \"3pu8tsqTW52aUtYFZN3g4A\")\neast_coast$coast &lt;- \"east\"\nwest_coast &lt;- get_playlist_audio_features(\"\", \"6lAOSVxxvGuEhPtZguaeav\")\nwest_coast$coast &lt;- \"west\"\nboth &lt;- rbind(east_coast, west_coast)\n\n####standardize and clean a bit\n\n\nboth &lt;- both %&gt;% select(c(\"coast\", \"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\ndata &lt;- both[,-1]\ncoast &lt;- both[,1]\ndata &lt;- data %&gt;% mutate_all(~(scale(.) %&gt;% as.vector))\nboth &lt;- cbind(coast, data)\n\n\nset.seed(1234)\ncontrol &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n# train logistic regression\nmodelglm &lt;- train(coast ~ ., data=both, method=\"glm\", trControl=control)\n\n# train knn\nmodelknn &lt;- train(coast ~ ., data=both, method=\"kknn\", trControl=control)\n\n# train nnet\nmodelnnet &lt;- train(coast ~ ., data=both, method=\"nnet\", trControl=control)\n\n# train the LVQ model\nmodelLvq &lt;- train(coast ~ ., data=both, method=\"lvq\", trControl=control)\n\n# train the GBM model\nmodelGbm &lt;- train(coast ~ ., data=both, method=\"gbm\", trControl=control)\n\n# train the SVM model\nmodelSvm &lt;- train(coast ~., data=both, method=\"svmRadial\", trControl=control)\n\n# train the random forest\nrandomforest &lt;- train(coast~., data=both, method=\"ranger\", trControl=control)\n\n\n# collect resamples\nresults &lt;- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm,knn=modelknn, nnet=modelnnet, glm=modelglm, rf=randomforest))\n\n# summarize the distributions\nsummary(results)\nbwplot(results)\n\n\n\n\nNow we can test our model on a dataset from outside of our initial training/testing stage:\n\nkendrick &lt;- get_artist_audio_features('kendrick lamar')\n\nkendrick &lt;- kendrick |&gt; select(c(\"acousticness\", \"energy\", \"instrumentalness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\nkendrick &lt;- kendrick %&gt;% mutate_all(~(scale(.) %&gt;% as.vector))\n\nkendrick_rf &lt;- predict(randomforest, newdata=kendrick)\nkendrick_knn &lt;- predict(modelknn, newdata=kendrick)\nkendrick_nnet &lt;- predict(modelnnet, newdata=kendrick)\ntable(kendrick_rf)\n\nkendrick_rf\neast west \n 107   35 \n\n# classified_data$beatles_knn &lt;- if_else(classified_data$beatles_knn == 2, \"Paul\", \"John\")\n# classified_data$beatles_rf&lt;- if_else(classified_data$beatles_rf == 2, \"Paul\", \"John\")\nclassified_data &lt;- as.data.frame(cbind(kendrick_rf, kendrick_knn, kendrick_nnet, kendrick$track_name))"
  }
]