[
  {
    "objectID": "class_notes/week_9.html",
    "href": "class_notes/week_9.html",
    "title": "Week 9: Recommender Systems",
    "section": "",
    "text": "Talking about Recommender Systems\nWhat kind of data would you need for your project?\nMonday: Basic Recommender Code\nWednesday: Including Spotify Features"
  },
  {
    "objectID": "class_notes/week_9.html#the-cold-start-problem",
    "href": "class_notes/week_9.html#the-cold-start-problem",
    "title": "Week 9: Recommender Systems",
    "section": "The Cold Start Problem",
    "text": "The Cold Start Problem\nThis was all random data. How will you get data for your project? What will it look like for your model to be accurate?"
  },
  {
    "objectID": "class_notes/week_9.html#two-spotify-discover-playlists",
    "href": "class_notes/week_9.html#two-spotify-discover-playlists",
    "title": "Week 9: Recommender Systems",
    "section": "Two Spotify Discover Playlists",
    "text": "Two Spotify Discover Playlists\nWhat if I take my own discover playlist, and find someone else’s and see what it might recommend?\nWe can start by getting data from playlists. Put in your own Spotify data here. I was having some cache issues with my client id, so I’m using the MemoryCacheHandler function from Spotipy.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nfrom surprise import Dataset, Reader, KNNBasic\nimport pandas as pd\nfrom spotipy.cache_handler import MemoryCacheHandler\n\n\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id = 'YOUR OWN CLIENT ID',\n    client_secret = 'YOUR OWN CLIENT SECRET',\n    cache_handler=MemoryCacheHandler()\n)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\nHere’s a function that grabs the audio data from Spotify, using spotipy and pandas:\n\n# Function to get playlist tracks and audio features\ndef get_playlist_data(playlist_id):\n    results = sp.playlist_tracks(playlist_id)\n    tracks = results['items']\n    \n    data = []\n    for track in tracks:\n        track_id = track['track']['id']\n        audio_features = sp.audio_features(track_id)[0]\n        \n        data.append({\n            'track_id': track_id,\n            'acousticness': audio_features['acousticness'],\n            'danceability': audio_features['danceability'],\n            'energy': audio_features['energy'],\n            'instrumentalness': audio_features['instrumentalness'],\n            'liveness': audio_features['liveness']\n        })\n    \n    return pd.DataFrame(data)\n\nAnd we can add all of the Spotify URIs for the playlist:\n\n# Get data for both playlists\nplaylist1_id = '37i9dQZEVXcIDdjTZ1FKnJ'\nplaylist2_id = '2VYB4MjPYJx3ZOkrjyUvsx'\n\ndf1 = get_playlist_data(playlist1_id)\ndf2 = get_playlist_data(playlist2_id)\n\nNow we will read it into Surprise with the Reader function:\n\n# Assign user IDs (1 for playlist1, 2 for playlist2)\ndf1['user_id'] = 'Dannah Shanalee'\ndf2['user_id'] = 'Hannah Ashlee'\n\n# Combine dataframes\ncombined_df = pd.concat([df1, df2], ignore_index=True)\n\n# Prepare data for Surprise\nreader = Reader(rating_scale=(0, 1))\ndata = Dataset.load_from_df(combined_df[['user_id', 'track_id', 'acousticness']], reader)\n\nHere we have a function that gets the track information (name, artist, and id) to print it later (it was only printing URIs), and then recommend 5 tracks from the other playlist:\n\ndef get_track_info(track_id):\n    try:\n        track_info = sp.track(track_id)\n        return {\n            'name': track_info['name'],\n            'artist': track_info['artists'][0]['name'],\n            'id': track_id\n        }\n    except:\n        return {'name': 'Unknown', 'artist': 'Unknown', 'id': track_id}\n\ndef get_recommendations(track_id, model, n=5):\n    inner_id = model.trainset.to_inner_iid(track_id)\n    neighbors = model.get_neighbors(inner_id, k=n)\n    raw_neighbors = [model.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n    return [get_track_info(track_id) for track_id in raw_neighbors]\n\n# Example: Get recommendations for a track\nsample_track = combined_df['track_id'].iloc[0]\nsample_track_info = get_track_info(sample_track)\nrecommendations = get_recommendations(sample_track, algo)\n\nprint(f\"Recommendations for '{sample_track_info['name']}' by {sample_track_info['artist']}:\")\nfor rec in recommendations:\n    print(f\"- '{rec['name']}' by {rec['artist']} (ID: {rec['id']})\")\n\nAnd we get five new recommendations:\n\nRecommendations for 'Lily Pad on Your Doorstep' by Don't Stop or We'll Die:\n- 'Amazing Glow' by Pernice Brothers (ID: 6ViAxolMqWD1d6JrqgPZKc)\n- 'Rang Tang Ring Toon' by Mountain Man (ID: 2HZRjBrPeo3HwyZVUZxK62)\n- 'Amsterdam' by Guster (ID: 3fv9cBtpMOaFaIAO4uVRBV)\n- 'Bernadette' by Elle Cordova (ID: 4NFhb4AOPBulDFgxyoXaLH)\n- 'Tire Swing' by Kimya Dawson (ID: 0vbhRDi46TDNHkhKbZa81B)\nAfter working through this example, in class, we realized that it wasn’t really doing what we thought it was doing. We thought it was picking the top tracks from the other playlist, but in fact it was taking a single track and picking five similar tracks from that same playlist.\nSo for Wednesday we want to do a few things:\n\nLook at two playlists, and use the entire set of songs to predict a single new song from playlist 1 for the user of playlist 2.\nWe want to compare a few different models and see what this looks like, comparing the accuracy as needed."
  },
  {
    "objectID": "class_notes/week_8.html",
    "href": "class_notes/week_8.html",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "",
    "text": "Today:\n\nTalk about discussion posts\nQuick aside on grabbing specific features from Timbre\nLooking at lyric analysis\n\nWednesday:\n\nProjects due this week (Wednesday Night at 11:59)\nBegin discussion on Recommender Systems (the topic of project 4)\n\n\n\n## Loading some libraries\nlibrary(tidytext)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(textdata)\nlibrary(DT)\nlibrary(spotifyr)\n\n\nAttaching package: 'spotifyr'\n\nThe following object is masked from 'package:tidytext':\n\n    tidy\n\nlibrary(compmus)"
  },
  {
    "objectID": "class_notes/week_8.html#what-dataset-to-use",
    "href": "class_notes/week_8.html#what-dataset-to-use",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "What dataset to use?",
    "text": "What dataset to use?\nIt’s quite difficult to get lyrics from online sources at the moment. Many of the R interfaces for the Genius API seem to be quite deprecated. Kaggle does have many datasets available, including about 10GB of Genius data. For today, let’s just look at a small dataset of Taylor Swift lyrics."
  },
  {
    "objectID": "class_notes/week_8.html#what-is-tf-idf",
    "href": "class_notes/week_8.html#what-is-tf-idf",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "What is TF-IDF?",
    "text": "What is TF-IDF?\n\ntaylor &lt;- read.csv(\"~/Downloads/taylor_swift_lyrics.csv\")\n\nThere’s been some interesting stuff done on lyrics already. We can modify an existing tutorial on Ed Sheeran for today’s discussion.\n\ntaylor_word_count &lt;- taylor %&gt;%\n  unnest_tokens(word, lyric) %&gt;%\n  group_by(track_title, album) %&gt;%\n  summarise(num_words = n()) %&gt;%\n  arrange(desc(num_words)) \n\n`summarise()` has grouped output by 'track_title'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "class_notes/week_8.html#filtering-words",
    "href": "class_notes/week_8.html#filtering-words",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Filtering Words",
    "text": "Filtering Words\nThe anti_join function from the tidyverse basically joins all columns that don’t match something. Here, it’s the words that tidytext has defined has stop words.\n\nwords_filtered &lt;- taylor %&gt;%\n  unnest_tokens(word, lyric) %&gt;%\n  anti_join(stop_words) %&gt;%\n  distinct()\n\nJoining with `by = join_by(word)`"
  },
  {
    "objectID": "class_notes/week_8.html#plotting-the-most-words",
    "href": "class_notes/week_8.html#plotting-the-most-words",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Plotting the most words",
    "text": "Plotting the most words\n\nwords_filtered %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(30) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot() +\n    geom_col(aes(word, n), fill = 'light blue') +\n    theme(legend.position = \"none\", \n          plot.title = element_text(hjust = 0.5),\n          panel.grid.major = element_blank()) +\n    xlab(\"\") + \n    ylab(\"Song Count\") +\n    ggtitle(\"Most Frequently Used Words in Lyrics\") +\n    coord_flip() +\n     theme_bw()\n\nSelecting by n"
  },
  {
    "objectID": "class_notes/week_8.html#word-counts",
    "href": "class_notes/week_8.html#word-counts",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Word Counts",
    "text": "Word Counts\n\ntaylor_word_count %&gt;%\n  ggplot() +\n    geom_density(aes(x = num_words, fill = album), alpha = 0.5, position = 'stack') +\n    ylab(\"Song Density\") + \n    xlab(\"Word Count per Song\") +\n    ggtitle(\"Word Count Distribution\") +\n    theme(plot.title = element_text(hjust = 0.5),\n          legend.title = element_blank(),\n          panel.grid.minor.y = element_blank()) +\n          theme_bw()\n\n\n\n\n\nWords over Time\nHere we can see the changing word count over time in Taylor Swift’s albums.\n\nwords &lt;- words_filtered %&gt;%\n  group_by(year) %&gt;%\n  count(word, year, sort = TRUE) %&gt;%\n  slice(seq_len(8)) %&gt;%\n  ungroup() %&gt;%\n  arrange(year, n) %&gt;%\n  mutate(row = row_number())\n\nwords %&gt;%\n  ggplot(aes(row, n, fill = year)) +\n    geom_col(show.legend = NULL) +\n    labs(x = NULL, y = \"Song Count\") +\n    ggtitle(\"Words Across the Year\") + \n    theme_bw() +  \n    facet_wrap(~year, scales = \"free\") +\n    scale_x_continuous(  # This handles replacement of row \n      breaks = words$row, # notice need to reuse data frame\n      labels = words$word) +\n    coord_flip()\n\nWarning: `show.legend` must be a logical vector."
  },
  {
    "objectID": "class_notes/week_8.html#words-by-album",
    "href": "class_notes/week_8.html#words-by-album",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Words by Album",
    "text": "Words by Album\nThis basically does the same as before, but breaks it down by album and not year.\n\nwords &lt;- words_filtered %&gt;%\n  group_by(album) %&gt;%\n  count(word, album, sort = TRUE) %&gt;%\n  slice(seq_len(8)) %&gt;%\n  ungroup() %&gt;%\n  arrange(album, n) %&gt;%\n  mutate(row = row_number())\n\nwords %&gt;%\n  ggplot(aes(row, n, fill = album)) +\n    geom_col(show.legend = NULL) +\n    labs(x = NULL, y = \"Song Count\") +\n    ggtitle(\"Words Across the Album\") + \n    theme_bw() +  \n    facet_wrap(~album, scales = \"free\") +\n    scale_x_continuous(  # This handles replacement of row \n      breaks = words$row, # notice need to reuse data frame\n      labels = words$word) +\n    coord_flip()\n\nWarning: `show.legend` must be a logical vector."
  },
  {
    "objectID": "class_notes/week_8.html#tf-idf",
    "href": "class_notes/week_8.html#tf-idf",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "TF-IDF",
    "text": "TF-IDF\nWhere it starts to get interesting is when we can begin to employ metrics of frequency in relation to other data points. The Term Infrequency-Inverse Document Frequency (TF-IDF) metric is a good starting point for that.\nBasically, TF-IDF measures not just how often a word occurs, but how often it occurs in relation to other collections. So if there’s a word that occurs everywhere (like “love”), it’s not really weighted as highly.\n\ntfidf_words_album &lt;- taylor %&gt;%\n  unnest_tokens(word, lyric) %&gt;%\n  distinct() %&gt;%\n  count(album, word, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  bind_tf_idf(word, album, n) %&gt;%\n  arrange(desc(tf_idf))\n\nThis grabs the top 10 words per album. Notice that the tf_idf metric is simply the product of tf (term frequency) multiplied by idf (inverse document frequency).\n\ntop_tfidf_words_album &lt;- tfidf_words_album %&gt;% \n  group_by(album) %&gt;% \n  slice(seq_len(10)) %&gt;%\n  ungroup() %&gt;%\n  arrange(album, tf_idf) %&gt;%\n  mutate(row = row_number())  \n\ntop_tfidf_words_album %&gt;% datatable(filter=\"top\")\n\n\n\n\n\n\nWe can plot the data like so:\n\ntop_tfidf_words_album %&gt;%\n  ggplot(aes(x = row, tf_idf, fill = album)) +\n  geom_col(show.legend = NULL) +\n  labs(x = NULL, y = \"TF-IDF\") + \n  ggtitle(\"Important Words by Album (as measured by TF-IDF)\") +\n  theme_bw() +  \n  facet_wrap(~album,\n             scales = \"free\") +\n  scale_x_continuous(  # this handles replacement of row \n    breaks = top_tfidf_words_album$row, # notice need to reuse data frame\n    labels = top_tfidf_words_album$word) +\n  coord_flip()\n\nWarning: `show.legend` must be a logical vector."
  },
  {
    "objectID": "class_notes/week_8.html#sentiment-analysis",
    "href": "class_notes/week_8.html#sentiment-analysis",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nWe can then run sentiment analysis on the lyrics after unnesting and getting rid of stop words.\n\ntaylor_words &lt;- taylor %&gt;%\n  ##this breaks the lyrics up into words.\n  unnest_tokens(word, lyric) %&gt;% \n  ## the stop words come from tidytext.\n  anti_join(stop_words) \n\nJoining with `by = join_by(word)`\n\n\nWe will start with the bing classification (named after the PI of the research group, Liu Bing). This classifies words as either positive or negative sentiment. See this PsychStat book for more on these metrics.\n\ntaylor_bing &lt;- taylor_words %&gt;%\n  inner_join(get_sentiments(\"bing\"))\n\nJoining with `by = join_by(word)`\n\n\nWe can also run the nrc word list, which puts words into positive or negative categories, but also uses 8 other emotions, including:\n\nanger\nanticipation\ndisgust\nfear\njoy\nsadness\nsurprise\ntrust\n\nSee the “sentiment” column below in the table. To what extent do we agree with these categories? Do they seem useful to you?\n\ntaylor_nrc &lt;- taylor_words %&gt;%\n  inner_join(get_sentiments(\"nrc\"))\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6 of `x` matches multiple rows in `y`.\nℹ Row 6230 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntaylor_nrc %&gt;% datatable(filter =\"top\") \n\n\n\n\n\n\nWe can clean this up by getting rid of the positive and negative emotions, if we’d like:\n\ntaylor_nrc_no_pos_neg &lt;- taylor_words %&gt;%\n  inner_join(get_sentiments(\"nrc\")) %&gt;%\n  filter(!sentiment %in% c(\"positive\", \"negative\"))\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6 of `x` matches multiple rows in `y`.\nℹ Row 6230 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntaylor_nrc_no_pos_neg %&gt;% datatable(filter =\"top\")\n\n\n\n\n\n\nAnd here we can get have only the listing of words rated as “positive” or “negative”.\n\ntaylor_nrc_pos_neg &lt;- taylor_words %&gt;%\n  inner_join(get_sentiments(\"nrc\")) %&gt;%\n  filter(sentiment %in% c(\"positive\", \"negative\"))\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6 of `x` matches multiple rows in `y`.\nℹ Row 6230 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntaylor_nrc_pos_neg %&gt;% datatable(filter =\"top\")\n\n\n\n\n\n\nAnd here we can plot everything:\n\nnrc_plot &lt;- taylor_nrc %&gt;%\n  group_by(sentiment) %&gt;%\n  summarise(word_count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(sentiment = reorder(sentiment, word_count)) %&gt;%\n  ggplot(aes(sentiment, word_count, fill = -word_count)) +\n  geom_col() +\n  theme_bw() +\n  labs(x = NULL, y = \"Word Count\") +\n  ggtitle(\"NRC Sentiment\") +\n  coord_flip()\nnrc_plot + guides(fill=FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\nWe can write a function to look at the various sentiments of a tune, which we can then join with other data.\n\ncalculate_sentiment &lt;- function(df){\n  df %&gt;%\n    group_by(track_title) %&gt;%\n    unnest_tokens(word, lyric) %&gt;%\n    left_join(get_sentiments(\"nrc\"), multiple = \"all\") %&gt;%\n    filter(!is.na(sentiment)) %&gt;%\n    count(sentiment) %&gt;%\n    pivot_wider(names_from = sentiment, values_from = n) %&gt;%\n    mutate(sentiment = positive - negative)\n}\n\ncalculate_sentiment(taylor) \n\nJoining with `by = join_by(word)`\n\n\nWarning in left_join(., get_sentiments(\"nrc\"), multiple = \"all\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 14 of `x` matches multiple rows in `y`.\nℹ Row 7687 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 94 × 12\n# Groups:   track_title [94]\n   track_title  anger anticipation disgust  fear   joy negative positive sadness\n   &lt;chr&gt;        &lt;int&gt;        &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n 1 ...Ready fo…     3           10       4     9     8        9        9       4\n 2 22               9           13       9     7    11       12       14      10\n 3 A Perfectly…     4            8       4     4    12        5       14       5\n 4 A Place In …     1            4       1     1     3        4        4       2\n 5 All Too Well     5            6       3     2     4       11        7       6\n 6 All You Had…    NA           NA       1    NA     2        5        2       4\n 7 Back To Dec…     1           17      NA     9    16        9       22       7\n 8 Bad Blood       22            3      20    22    27       24       27      22\n 9 Begin Again     NA            4      NA    NA     6        1       13       1\n10 Better Than…     9           14       6    12    11       14       17       5\n# ℹ 84 more rows\n# ℹ 3 more variables: surprise &lt;int&gt;, trust &lt;int&gt;, sentiment &lt;int&gt;\n\n\nAnd we can combine it with Spotify data!\nLet’s get the spotify data…\n\nts &lt;- get_artist_audio_features(\"Taylor Swift\")\n# ts &lt;- get_playlist_audio_fea\"tures(\"spotify\", \"3dgpO6mDWzdpMhyttrVi9t?si=b4f962245cb7468a\")\n\nAnd now we can combine the data together…\n\nts_basic_audio &lt;- ts %&gt;%\n  select(track_name, danceability:tempo, album_name) %&gt;%\n  rename(track_title = track_name)\n\njoined_ts &lt;- calculate_sentiment(taylor) %&gt;%\n  left_join(ts_basic_audio)\n\nJoining with `by = join_by(word)`\n\n\nWarning in left_join(., get_sentiments(\"nrc\"), multiple = \"all\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 14 of `x` matches multiple rows in `y`.\nℹ Row 7687 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nJoining with `by = join_by(track_title)`\n\njoined_ts %&gt;% \n  datatable(filter=\"top\")\n\n\n\n\n\n\nHere’s the code to plot the overall positive/negative of the tune:\n\nts &lt;- get_artist_audio_features(\"Taylor Swift\")\n\n\nts_basic_audio &lt;- ts %&gt;%\n  select(track_name, danceability:tempo) %&gt;%\n  rename(track_title = track_name) |&gt;\n  distinct()\n\njoined_ts &lt;- calculate_sentiment(taylor) %&gt;%\n  left_join(ts_basic_audio, multiple = \"all\")\n\nJoining with `by = join_by(word)`\n\n\nWarning in left_join(., get_sentiments(\"nrc\"), multiple = \"all\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 14 of `x` matches multiple rows in `y`.\nℹ Row 7687 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nJoining with `by = join_by(track_title)`\n\njoined_ts %&gt;%\n  select(track_title, positive, negative) %&gt;%\n  pivot_longer(cols = positive:negative, names_to = \"sentiment\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = reorder(track_title, value), y = value, color = sentiment)) +\n  geom_point() +\n  coord_flip() +\n  theme_classic() +\n  labs(title = \"Taylor's Sentiments Across Tracks\",\n       y = 'Sentiment Value',\n       x = \"Track\")\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "class_notes/week_8.html#blur-and-oasis",
    "href": "class_notes/week_8.html#blur-and-oasis",
    "title": "Week 8: Text, TF-IDF, etc.",
    "section": "Blur and Oasis",
    "text": "Blur and Oasis\nThis data is taken from the “Million Song Dataset” from Spotify.\n\noasis &lt;- read.csv(\"oasis.csv\")\nblur &lt;- read.csv(\"blur.csv\")\n\nAnd we can begin getting words like this:\n\noasis_words &lt;- oasis %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  distinct()\n\nContinuing with a summary like so…\n\nfull_word_count &lt;- oasis %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  group_by(track_title) %&gt;%\n  summarise(num_words = n()) %&gt;%\n  arrange(desc(num_words))"
  },
  {
    "objectID": "class_notes/week_9.html#importing-the-libraries-and-the-data",
    "href": "class_notes/week_9.html#importing-the-libraries-and-the-data",
    "title": "Week 9: Recommender Systems",
    "section": "Importing the Libraries and the Data",
    "text": "Importing the Libraries and the Data\nFirst, I’m going to import the spotipy, pandas, and surprise. I’m also going to import specific authentication and cache things with spotipy, and some models with Surprise.\n\nimport spotipy\nimport os\nfrom surprise import Dataset, Reader, KNNBasic, SVD\nimport pandas as pd\nfrom spotipy.cache_handler import MemoryCacheHandler\nfrom surprise.model_selection import cross_validate\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nI’ve stored my id stuff locally so you can’t add things to my playlists…\n\nimport os\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Access environment variables\nclient_id = os.getenv('SPOTIFY_CLIENT_ID')\nclient_secret = os.getenv('SPOTIFY_CLIENT_SECRET')\n\n# Set up Spotify client credentials\nsp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=client_id, client_secret=client_secret))\n\n\nimport os\n\nprint(os.getenv('SPOTIFY_CLIENT_ID'))\nprint(os.getenv('SPOTIFY_CLIENT_SECRET'))\n\nNow that those are loaded, let’s grab the data from two playlists from Spotify.\nWe will start with a function that takes the playlist uri as an argument, grabs the audio features from the tracks:\n\n# Function to get playlist tracks and audio features\ndef get_playlist_data(playlist_id):\n    results = sp.playlist_tracks(playlist_id)\n    tracks = results['items']\n    \n    data = []\n    for track in tracks:\n        track_id = track['track']['id']\n        audio_features = sp.audio_features(track_id)[0]\n        \n        data.append({\n            'track_id': track_id,\n            'acousticness': audio_features['acousticness'],\n            'danceability': audio_features['danceability'],\n            'energy': audio_features['energy'],\n            'instrumentalness': audio_features['instrumentalness'],\n            'liveness': audio_features['liveness'],\n            'mode': audio_features['mode'],\n            'tempo': audio_features['tempo'],\n\n\n        })\n    \n    return pd.DataFrame(data)\n\nWe can use this function and grab the information from the playlist with the following:\n\n# Get data for both playlists\nplaylist1_id = '37i9dQZEVXcIDdjTZ1FKnJ'\nplaylist2_id = '2VYB4MjPYJx3ZOkrjyUvsx'\n\ndf1 = get_playlist_data(playlist1_id)\ndf2 = get_playlist_data(playlist2_id)\n\nLet’s see what kind of data we have from this:\n\ndf1\n\nNote how it gets the track URI, and then all of the variables we asked for. All of these are continuous, with the exception of mode, which is a binary (0 or 1).\nWe can add our user data here:\n\n# Assign user IDs (1 for playlist1, 2 for playlist2)\ndf1['user_id'] = 'Dannah Shanalee'\ndf2['user_id'] = 'Hannah Ashlee'\n\nNow we can see the data has specific user names (here only the first five rows).\n\ndf1.head()\n\nWe can combine the two dataframes into one here:\n\ncombined_df = pd.concat([df1, df2], ignore_index=True)\ncombined_df['user_id'].value_counts()\n\nHere we see (using the value_counts function in pandas), that there are 100 songs in Hannah Ashlee’s list, and 30 in mine. Not ideal, but we will try to see where it takes us.\nNow we will load the data into the surprise toolkkit, using the Reader function. The rating_scale option gives it a range of ratings. Here it’s just 0 or 1, as it’s if it’s present in a playlist or not, but it could also be from 1 to 5 (if it’s reviews) or something like that.\n\nreader = Reader(rating_scale=(0, 1))\n\nThe load_from_df function in surprise is meant to accept three things: the user id, the item id, and a rating. Here, we don’t really have a rating, which again isn’t ideal. So here I’m just adding in a third thing (acousticness).\n\ndata = Dataset.load_from_df(combined_df[['user_id', 'track_id', 'acousticness']], reader)"
  },
  {
    "objectID": "class_notes/week_9.html#building-an-item-item-recommender-system",
    "href": "class_notes/week_9.html#building-an-item-item-recommender-system",
    "title": "Week 9: Recommender Systems",
    "section": "Building an Item-Item Recommender System",
    "text": "Building an Item-Item Recommender System\nHere I’m going to look at the sim_options argument for surprise, which is short for “similarity options”.\nThere are a few options to try for the name argument:\n\ncosine give us cosine similarity\nmsd gives use the mean squared difference similarity\npearson gives us the pearson correlation coefficient\npearson_baseline gives us the pearson correlation coefficient with a baseline estimate for ratings, which is meant to account for the overall average rating, and any user or item bias.\n\nFor now, we can just use pearson, but it’s worth comparing:\n\n## I'm going to give options for the model here, including specific model, and whether it's item based or user based.\nsim_options = {\n    'name': 'msd',\n    'user_based': False  # Item-based similarity\n}\n\nHere is my code for implementing a K-nearest neighbor:\n\ndans_algorithm = KNNBasic(sim_options=sim_options)\ntrainset = data.build_full_trainset()\ndans_algorithm.fit(trainset)\nsim_matrix = dans_algorithm.compute_similarities()\n\nThis gives us a similarity matrix between items. Here I’ll convert it to a pandas dataframe:\n\nsim_matrix\n# Convert to a pandas DataFrame\ndf_sim = pd.DataFrame(sim_matrix)\ndf_sim\n\nThis is telling me pairwise similarities using msd between the songs in the columns and the songs in the rows.\nNow that we have similarities, and compare between lists."
  },
  {
    "objectID": "class_notes/week_9.html#using-data-from-one-list-to-recommend-to-another",
    "href": "class_notes/week_9.html#using-data-from-one-list-to-recommend-to-another",
    "title": "Week 9: Recommender Systems",
    "section": "Using Data from One List to Recommend to Another",
    "text": "Using Data from One List to Recommend to Another\nLet’s start with a function to grab the track info based on the URI. This isn’t necessary, but if we want to be able to interpret anything past the URI, we should do this.\nNOTE: I kept getting errors because of Unknown IDs, it would choke the Spotify API up here. the try and except are meant to get around that.\n\ndef get_track_info(track_id):\n    try:\n        track_info = sp.track(track_id)\n        return {\n            'name': track_info['name'],\n            'artist': track_info['artists'][0]['name'],\n            'id': track_id\n        }\n    except:\n        return {'name': 'Unknown', 'artist': 'Unknown', 'id': track_id}\n\nNow we can take those original track URIs and write a function to get the recommendations. Here, we get the “inner id”, which is basically an internal representation used by surprise. Then we get the neighbors.\nA couple of other things:\n\nk is referring to the amount of neighbors; here I’ve set it to 10.\n\n\ndef get_recommendations(track_id, model, n=10):\n    inner_id = model.trainset.to_inner_iid(track_id)\n    neighbors = model.get_neighbors(inner_id, k=n)\n    raw_neighbors = [model.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n    return [get_track_info(track_id) for track_id in raw_neighbors]\n\nHere is code to get the recommendations for myself based on this other playlist:\n\n# Get recommendations for the user from playlist 2\nuser_id = 'Dannah Shanalee'\nplaylist2_tracks = df2['track_id'].tolist()\nrecommendations = []\nfor track_id in playlist2_tracks:\n    recs = get_recommendations(track_id, dans_algorithm, n=10)\n    recommendations.extend(recs)\n\nHere are the top 10 recommendations in a pandas dataframe:\n\npd.DataFrame(recommendations).head(10)\n\nNow it might be worth using the cross_validate function to see how well this did.\nIn the code below, I use a 5-fold cross validation. Just as a reminder, this does the following steps:\n\nPartitions the data into 5 “folds”\nIt then trains the data on 4 folds, and tests it on the other.\n\nThen, I look at both “RMSE” (Root Mean Square Error) and “MAE” (Mean Absolute Error).\n\nresults = cross_validate(dans_algorithm, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\nAnd as a pandas dataframe:\n\npd.DataFrame(results)\n\nI had to look up what “good” RMSE numbers might be for this, and it seems that with a binary rating (as we have here) or lower is considered “good”, so I think we’ve done a decent job here."
  }
]