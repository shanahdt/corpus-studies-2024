[
  {
    "objectID": "class_notes/week_6.html",
    "href": "class_notes/week_6.html",
    "title": "Week 5: Regression and Clustering",
    "section": "",
    "text": "This week, we will be going over:\n\ntesting some basic hypotheses of continuous data with linear regression\ntesting some basic hypotheses of categorical data with logistic regressions\nevaluating model fits"
  },
  {
    "objectID": "class_notes/week_6.html#looking-at-tempo-over-time",
    "href": "class_notes/week_6.html#looking-at-tempo-over-time",
    "title": "Week 5: Regression and Clustering",
    "section": "Looking at tempo over time",
    "text": "Looking at tempo over time\nWe can start by eyeballing the data. Here is how we’d do it with base R (no ggplot/tidyverse):\n\nplot(tempo ~ album_release_year, data=billy_joel)\nabline(lm(tempo ~ album_release_year, data=billy_joel), col=\"red\")\n\n\n\n\nIf we’d like to use ggplot it can give us some confidence bars (the default here is a 95% confidence interval):\n\nggplot(billy_joel, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSo it looks promising. We can run a linear regression with a simple lm command. Here we can get a summary of the model pretty easily, as well.\n\nsummary(lm(tempo ~ album_release_year, data=billy_joel))\n\n\nCall:\nlm(formula = tempo ~ album_release_year, data = billy_joel)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.632 -25.836  -0.508  23.329  68.277 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        -53.02885  187.12869  -0.283    0.777\nalbum_release_year   0.08637    0.09386   0.920    0.358\n\nResidual standard error: 30.61 on 355 degrees of freedom\nMultiple R-squared:  0.002379,  Adjusted R-squared:  -0.0004309 \nF-statistic: 0.8467 on 1 and 355 DF,  p-value: 0.3581\n\n\nSo, as we can see from the results here, it’s not significant (p &gt; .001), but it really doesn’t account for much of the variance (an adjusted R-squared of -.0004).\n\nPost-Hoc Analyses\nPerhaps we can look at how other variables might be predictive of the year of the recording.\nLet’s look at how tempo, danceability, valence, speechiness, and energy might improve the model.\n\nsummary(lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=billy_joel))\n\n\nCall:\nlm(formula = album_release_year ~ tempo + danceability + valence + \n    speechiness + energy, data = billy_joel)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.376 -10.790  -0.305  10.294  36.338 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2002.16180    4.69463 426.479  &lt; 2e-16 ***\ntempo          -0.02356    0.02566  -0.918    0.359    \ndanceability  -37.87836    7.40058  -5.118 5.09e-07 ***\nvalence       -19.21143    4.59097  -4.185 3.61e-05 ***\nspeechiness     1.47237    5.32909   0.276    0.782    \nenergy         30.68677    3.27153   9.380  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.24 on 351 degrees of freedom\nMultiple R-squared:  0.331, Adjusted R-squared:  0.3214 \nF-statistic: 34.73 on 5 and 351 DF,  p-value: &lt; 2.2e-16\n\n\nSo we have a more predictive model, with an adjusted R-squared of about .32.\nThere are some remaining questions, however. Firstly, is there covariance at play?\nWe can use the variance inflation factor (VIF) to look at this. Typically, each variable should be less than 5.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nbilly_joel_model &lt;- lm(album_release_year ~ tempo + danceability + valence + speechiness + energy, data=billy_joel)\nvif(billy_joel_model)\n\n       tempo danceability      valence  speechiness       energy \n    1.082621     1.633266     1.950562     1.034647     1.309249 \n\n\nA correlation plot can help us to visualize this a bit more.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\npiano_man &lt;- billy_joel |&gt; \n    select(c(\"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n  x &lt;- as.matrix(cor(piano_man))\n  round(x, 2)\n\n             acousticness liveness danceability loudness speechiness valence\nacousticness         1.00    -0.22        -0.25    -0.63        0.14   -0.42\nliveness            -0.22     1.00        -0.44     0.42        0.03   -0.14\ndanceability        -0.25    -0.44         1.00     0.00        0.06    0.56\nloudness            -0.63     0.42         0.00     1.00       -0.41    0.31\nspeechiness          0.14     0.03         0.06    -0.41        1.00   -0.10\nvalence             -0.42    -0.14         0.56     0.31       -0.10    1.00\n\n  corrplot(x, method=\"pie\")"
  },
  {
    "objectID": "class_notes/week_6.html#sidenote-isare-the-data-normal",
    "href": "class_notes/week_6.html#sidenote-isare-the-data-normal",
    "title": "Week 5: Regression and Clustering",
    "section": "Sidenote: Is/Are the data normal?",
    "text": "Sidenote: Is/Are the data normal?\nWe can test to see if the tempo data is normally distributed:\n\nqqnorm(billy_joel$tempo)\n\n\n\nhist(billy_joel$tempo)\n\n\n\nshapiro.test(billy_joel$tempo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  billy_joel$tempo\nW = 0.9806, p-value = 9.793e-05\n\nks.test(billy_joel$tempo, \"pnorm\")\n\nWarning in ks.test.default(billy_joel$tempo, \"pnorm\"): ties should not be\npresent for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  billy_joel$tempo\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nAt the moment, it doesn’t seem to be…"
  },
  {
    "objectID": "class_notes/week_6.html#stepwise-entry-regression",
    "href": "class_notes/week_6.html#stepwise-entry-regression",
    "title": "Week 5: Regression and Clustering",
    "section": "Stepwise Entry Regression",
    "text": "Stepwise Entry Regression\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=billy_joel), \n     direction=\"backward\"))\n\nStart:  AIC=1934.73\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n- tempo         1      65.0 77993 1933.0\n- speechiness   1     167.7 78095 1933.5\n&lt;none&gt;                      77928 1934.7\n- valence       1    1100.7 79028 1937.7\n- acousticness  1   11065.4 88993 1980.1\n- danceability  1   12221.7 90149 1984.7\n\nStep:  AIC=1933.03\nalbum_release_year ~ danceability + acousticness + speechiness + \n    valence\n\n               Df Sum of Sq   RSS    AIC\n- speechiness   1     170.5 78163 1931.8\n&lt;none&gt;                      77993 1933.0\n- valence       1    1239.9 79233 1936.7\n- acousticness  1   11006.3 88999 1978.2\n- danceability  1   12412.6 90405 1983.8\n\nStep:  AIC=1931.81\nalbum_release_year ~ danceability + acousticness + valence\n\n               Df Sum of Sq   RSS    AIC\n&lt;none&gt;                      78163 1931.8\n- valence       1    1372.5 79536 1936.0\n- acousticness  1   10836.8 89000 1976.2\n- danceability  1   12256.3 90419 1981.8\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + acousticness + \n    valence, data = billy_joel)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.038 -12.131  -1.921  11.315  37.401 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2031.308      3.449 588.878  &lt; 2e-16 ***\ndanceability  -54.265      7.294  -7.440 7.74e-13 ***\nacousticness  -19.751      2.823  -6.996 1.33e-11 ***\nvalence       -11.006      4.421  -2.490   0.0132 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.88 on 353 degrees of freedom\nMultiple R-squared:  0.2651,    Adjusted R-squared:  0.2588 \nF-statistic: 42.44 on 3 and 353 DF,  p-value: &lt; 2.2e-16\n\nsummary(step(lm(album_release_year ~ danceability + \n          tempo + acousticness + \n          speechiness + valence, data=billy_joel), \n     direction=\"forward\"))\n\nStart:  AIC=1934.73\nalbum_release_year ~ danceability + tempo + acousticness + speechiness + \n    valence\n\n\n\nCall:\nlm(formula = album_release_year ~ danceability + tempo + acousticness + \n    speechiness + valence, data = billy_joel)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-29.57 -12.43  -1.62  11.50  37.91 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2033.1925     5.0614 401.707  &lt; 2e-16 ***\ndanceability  -56.1173     7.5635  -7.419 8.94e-13 ***\ntempo          -0.0145     0.0268  -0.541   0.5887    \nacousticness  -20.1919     2.8601  -7.060 8.98e-12 ***\nspeechiness     4.8770     5.6116   0.869   0.3854    \nvalence       -10.0944     4.5335  -2.227   0.0266 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.9 on 351 degrees of freedom\nMultiple R-squared:  0.2673,    Adjusted R-squared:  0.2569 \nF-statistic: 25.61 on 5 and 351 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_notes/week_6.html#comparing-fits",
    "href": "class_notes/week_6.html#comparing-fits",
    "title": "Week 5: Regression and Clustering",
    "section": "Comparing Fits:",
    "text": "Comparing Fits:\nWe could construct a few models But how can we tell which of these is more predictable? For this, we can look at Akaike’s ‘An Information Criterion’(or AIC). The lower number is better, and any difference of more than 2 is considered a statistically significant difference fit.\n\ndance_model &lt;- lm(danceability ~ album_release_year, data=billy_joel)\nacoustic_model &lt;- lm(acousticness ~ album_release_year, data=billy_joel)\nspeech_model &lt;- lm(speechiness ~ album_release_year, data=billy_joel)\nvalence_model &lt;- lm(valence ~ album_release_year, data=billy_joel)\ntempo_model &lt;- lm(tempo ~ album_release_year, data=billy_joel)\ncombined_model &lt;- lm(album_release_year ~ tempo + acousticness + speechiness + valence, data=billy_joel)\n\n\nAIC(dance_model, \n    acoustic_model, \n    tempo_model,\n    speech_model, \n    valence_model, \n    combined_model)\n\n               df        AIC\ndance_model     3 -500.47993\nacoustic_model  3  163.75641\ntempo_model     3 3459.95593\nspeech_model    3 -365.56978\nvalence_model   3  -51.44052\ncombined_model  6 2999.86297\n\n\nThe combined model doesn’t seem to do terribly well here, which seems to muddy the question up a bit. There are some interesting points here with tempo, valence, acoustic, and speech, though."
  },
  {
    "objectID": "class_notes/week_6.html#is-a-linear-model-the-best-approach",
    "href": "class_notes/week_6.html#is-a-linear-model-the-best-approach",
    "title": "Week 5: Regression and Clustering",
    "section": "Is a linear model the best approach?",
    "text": "Is a linear model the best approach?\nWe can adjust how we are viewing this data (linear or polynomial fit) a bit with the code below. Here we can plot our data as a line:\n\nggplot(billy_joel, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 1, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd here we have it as a second order polynomial:\n\nggplot(billy_joel, aes(album_release_year, tempo) ) + geom_point() +\n  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE)) +\n  theme_bw()\n\n\n\n\nAnd we can compare fits here:\n\nlinear &lt;- lm(album_release_year ~ tempo, data = billy_joel)\npoly_2 &lt;- lm(album_release_year ~ tempo + I(album_release_year^2), data = billy_joel)\n\nAIC(linear, \n    poly_2)\n\n       df       AIC\nlinear  3  3052.039\npoly_2  4 -1041.695\n\n\nSo it looks like a polynomial fit is a better one, but still isn’t that great."
  },
  {
    "objectID": "class_notes/week_6.html#predicting-a-categorical-variable",
    "href": "class_notes/week_6.html#predicting-a-categorical-variable",
    "title": "Week 5: Regression and Clustering",
    "section": "Predicting a categorical variable",
    "text": "Predicting a categorical variable\nWhat does it look like to predict a categorical variable in R? We might explore this question by hypothesizing that musical features might be predictive of mode (labeled in Spotify as major or minor: 1 or 0).\nHere is what a binomial logistic regression would look like:\n\nbilly_joel.log &lt;- glm(mode ~ tempo + danceability + valence +\n                     speechiness + acousticness, family = binomial, data = billy_joel)\n\nAnd it looks like “speechiness” is the most predictive of mode here.\n\nsummary(billy_joel.log)\n\n\nCall:\nglm(formula = mode ~ tempo + danceability + valence + speechiness + \n    acousticness, family = binomial, data = billy_joel)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4202   0.3895   0.4728   0.5709   1.0682  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   4.413208   1.060553   4.161 3.17e-05 ***\ntempo        -0.006563   0.005367  -1.223   0.2214    \ndanceability -3.572894   1.499965  -2.382   0.0172 *  \nvalence       1.175420   0.882904   1.331   0.1831    \nspeechiness  -0.596981   0.877367  -0.680   0.4962    \nacousticness -1.336813   0.563678  -2.372   0.0177 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 289.22  on 356  degrees of freedom\nResidual deviance: 274.08  on 351  degrees of freedom\nAIC: 286.08\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can plot the log odds ratios as well:\n\nCI &lt;- exp(confint(billy_joel.log))[-1,]\n\nWaiting for profiling to be done...\n\nsjPlot::plot_model(billy_joel.log,\n                   axis.lim = c(min(CI), max(CI)),\n                   auto.label = F,\n                   show.values = T) +\n                   theme_bw()"
  },
  {
    "objectID": "class_notes/week_6.html#clustering",
    "href": "class_notes/week_6.html#clustering",
    "title": "Week 5: Regression and Clustering",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis is a form of statistical data analysis in which subsets (called “clusters”) are formed according to some notion of similarity. There are many different variants of cluster analysis, but most are hierarchical–in which low-level clusters are successively joined together to make larger clusters, and so on, until everything is clustered into one large group. The result is a cluster tree or dendrogram.\n\nHow does the R hclust function work?\nThe hclust function is part of the default package in R, and it clusters based on dissimilarities in the data. There are different algorithms it can use, but the default is Ward’s minimum variance. It requires some distance to be calculated first, so the dist function is used on the data. Again there are many options here, but the default is to simply calculate the Euclidean distance between the values.\nThe documentation states:\n\nThis function performs a hierarchical cluster analysis using a set of dissimilarities for the n objects being clustered. Initially, each object is assigned to its own cluster and then the algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster. At each stage distances between clusters are recomputed by the Lance–Williams dissimilarity update formula according to the particular clustering method being used.\n\nThe default is Ward’s minimum variance method, which:\n\naims at finding compact, spherical clusters. The complete linkage method finds similar clusters.\n\nAnother method is the “single linkage method”.\n\nThe single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy. The other methods can be regarded as aiming for clusters with characteristics somewhere between the single and complete link methods. Note however, that methods “median” and “centroid” are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012).\n\n\n# cluster demo modified from here: \n### https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/\nlibrary(tidyverse)\nlibrary(cluster)\nbeyonce &lt;- read.csv(\"../data/beyonce.csv\")\ntaylor &lt;- read.csv(\"../data/taylor.csv\")\n\n\ndf &lt;- beyonce |&gt; \n  filter(album_name == \"4\") |&gt;\n  select(c(\"track_name\", \"acousticness\", \"liveness\", \"danceability\", \"loudness\", \"speechiness\", \"valence\"))\n\n\ndf &lt;- df |&gt; distinct(track_name, .keep_all = TRUE)\n\n## cleaning up the data.\nz &lt;- df[,-c(1,1)]\n\n### getting means of each category.\nmeans &lt;- apply(z,2,mean)\n### getting standard deviation of each category.\nsds &lt;- apply(z,2,sd)\n\n### scales the data in the matrix.\nscaled_data &lt;- scale(z,center=means,scale=sds)\ndistance &lt;- dist(scaled_data)\n\nAnd we can plot the data like this:\n\n### helps with the size of the image.\npar(mar = c(5, 4, 4, 1))\n\n### creates the cluster\ndf.hclust &lt;- hclust(distance)\n\n### plots the data but with row numbers.\nplot(df.hclust)\n\n\n\n\nAnd we can add the track name like so:\n\nplot(df.hclust,labels=df$track_name,main='Default from hclust')\n\n\n\n\nWe can clean up the plot the be along a single x-axis with the hang argument.\n\nnodePar &lt;- list(lab.cex = 0.6, pch = c(NA, 19), \n                cex = 0.7, col = \"blue\")\nplot(df.hclust,hang=-1, labels=df$track_name,main='Default from hclust')"
  },
  {
    "objectID": "class_notes/week_6.html#which-track-belongs-to-which-cluster",
    "href": "class_notes/week_6.html#which-track-belongs-to-which-cluster",
    "title": "Week 5: Regression and Clustering",
    "section": "Which track belongs to which cluster?",
    "text": "Which track belongs to which cluster?\nIt might be helpful with this analysis to look at how each of the songs fits on the tree. We can use the cutree function, which “cuts a tree” from the cluster based on how many groups we ask it for.\nThe following code can tell us how many fall into each broader tree, assuming we think that the tree should be cut into three. Notice that the third branch is the most populous, with the second being the most sparsely populated.\n\nmember &lt;- cutree(df.hclust,3)\ntable(member)\n\nmember\n 1  2  3 \n 4  3 11 \n\n\nBut how is each category being weighted? The code below shows that acousticness and danceability do a fair bit of work in separating groups 1 and 3, and valence separates 1 and 2 from one another.\n\n##but how are these clusters calculated?\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6151964  0.2494703    0.4173500  0.4287478  -0.4406914\n     valence\n1 -1.2506018\n2  1.0310423\n3  0.1735709\n\n\nA slightly more even split occurs if we break it into four groups rather than three.\n\nmember &lt;- cutree(df.hclust,4)\ntable(member)\n\nmember\n1 2 3 4 \n4 3 9 2 \n\n\nAnd that how they’re split into four is a bit different from how we might split them into three, but danceability and acousticness still playing a strong role.\n\naggregate(scaled_data,list(member),mean)\n\n  Group.1 acousticness   liveness danceability   loudness speechiness\n1       1    1.2293770 -0.3037203   -1.5341655 -0.4289046   0.3971934\n2       2    0.6165508 -0.5097641    0.5152708 -1.0002027   1.0862774\n3       3   -0.6115250 -0.1547202    0.4044253  0.3613888  -0.5628163\n4       4   -0.6317177  2.0683279    0.4755110  0.7318636   0.1088705\n      valence\n1 -1.25060184\n2  1.03104230\n3  0.05196371\n4  0.72080353\n\n\n\nK-Means Clustering\nWe can also run a simple k-means clustering on the data. With this, we are clustering the data into k groups. R’s documentation explains it like so:\n\naims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized. At the minimum, all cluster centres are at the mean of their Voronoi sets (the set of data points which are nearest to the cluster centre).\n\nThere are a few algorithms to pick from. R uses the Hartigan and Wong (1979) algorithm by default.\n\n###split it into three groups\nset.seed(123)\nkc &lt;- kmeans(scaled_data,3)\n\n### add labels.\nrow.names(scaled_data) &lt;- df$track_name\n\n###get the shortest distance.\ndatadistshortset&lt;-dist(scaled_data,method = \"euclidean\")\n\nThe code below will cluster it based on this k-means clustering distance, and plot them into the amount of groups listed (here 3).\n\nhc1 &lt;- hclust(datadistshortset, method = \"complete\" )\npamvshortset &lt;- pam(datadistshortset,3, diss = FALSE)\n\nclusplot(pamvshortset, shade = FALSE,labels=2,col.clus=\"blue\",col.p=\"red\",span=FALSE,main=\"Cluster Mapping\",cex=1.2)"
  },
  {
    "objectID": "class_notes/week_6.html#example-1-looking-at-nirvanas-unplugged-in-new-york",
    "href": "class_notes/week_6.html#example-1-looking-at-nirvanas-unplugged-in-new-york",
    "title": "Week 5: Regression and Clustering",
    "section": "Example 1: Looking at Nirvana’s Unplugged in New York",
    "text": "Example 1: Looking at Nirvana’s Unplugged in New York\nI’m going to get the global features from Nirvana’s Unplugged in New York album:\n\n# Get the album tracks\nalbum_id &lt;- \"1To7kv722A8SpZF789MZy7\"  # This is the Spotify ID for \"MTV Unplugged In New York\"\n\n##get all the tracks.\nalbum_tracks &lt;- get_album_tracks(album_id)\n\n# Get all of the audo features.\naudio_features &lt;- get_track_audio_features(album_tracks$id)\n\n# combine the tracks with the features.\nnirvana_unplugged &lt;- left_join(album_tracks, audio_features, by = \"id\")\n\nThis gets lots of data, and I’m just interested in their global measures (tempo, danceability, liveness, etc.). Here, I’ve gone with column number rather than name, but the other version might be a bit easier/cleaner. Nevertheless, this is another way of doing it:\n\n### This subsets the data based on only the columns I want.\nnirvana_unplugged_data &lt;- nirvana_unplugged |&gt; \n  select(danceability, energy, loudness, speechiness, tempo, valence, acousticness, instrumentalness, energy)\n\n### I assign the track name column (30) with the rownames, to have a labeled cluster.\nrownames(nirvana_unplugged_data) &lt;- nirvana_unplugged$name\n\n\nhc &lt;- hclust(dist(nirvana_unplugged_data), method = \"complete\", members = NULL)\nplot(hc)\n\n\n\n\n\nPlotting the cluster\nTechnically, you could just use the plot function at this point, but there are some long title names, so I added these extra plot options to make the text smaller and increase the marins.\n\npar(cex=0.5, mar=c(5, 8, 4, 1))\nplot(hc, labels =  row.names(nirvana_unplugged_data),xlab=\"\", ylab=\"\", main=\"\", sub=\"\", axes=FALSE)\npar(cex=1)\ntitle(xlab=\"tunes\", ylab=\"height\", main=\"Nirvana unplugged\")\naxis(2)\n\n\n\n\nThis plot is a bit strange now, as we have a pretty big negative number on the y-axis. Nevertheless, we see some cool things. The songs written by the Meat Puppets cluster together, for example.\n\n\nk-means clustering\nOur next type of clustering analysis is be a k-means cluster. We will start off by using a scree plot to see how many clusters we should use. There are a number of ways of analyzing where an “elbow” on this plot might be, but many people actually just eyeball it.\n\ntitles &lt;- nirvana_unplugged$name\nnirvana_unplugged_data &lt;- scale(nirvana_unplugged_data[,-10]) # standardize variables\nnirvana_unplugged_data &lt;- as.data.frame(nirvana_unplugged_data) # standardize variables\n\n\n# Determine number of clusters\nwss &lt;- (nrow(nirvana_unplugged_data)-1)*sum(apply(nirvana_unplugged_data,2,var))\n\nfor (i in 1:8) wss[i] &lt;- sum(kmeans(nirvana_unplugged_data, \n                                    centers=i)$withinss)\n  plot(1:8, wss, type=\"b\", xlab=\"Number of Clusters\",\n    ylab=\"Within groups sum of squares\")\n\n\n\n\nAnd now we can look at the k-means clustering based on however many clusters we think are necessary.\n\n# K-Means Cluster Analysis\nfit &lt;- kmeans(nirvana_unplugged_data, 3) # 3 cluster solution\n# get cluster means \naggregate(nirvana_unplugged_data,by=list(fit$cluster),FUN=mean)\n\n  Group.1 danceability     energy   loudness speechiness      tempo     valence\n1       1   -0.5625808  0.4212425  0.5351353   0.5578725  0.6479399 -0.04483405\n2       2   -0.7610397 -0.9842223 -1.5210813   0.1617592 -1.3717591 -0.91001724\n3       3    0.8493372  0.1410757  0.3145946  -0.5457733  0.1459297  0.49237033\n  acousticness instrumentalness\n1   -1.0817044       -0.3337712\n2    0.7858082        0.9230788\n3    0.5085162       -0.1833967\n\n# append cluster assignment\nunplugged_appended &lt;- data.frame(nirvana_unplugged_data, fit$cluster)\n\n\nrownames(unplugged_appended) = titles\nclusplot(unplugged_appended, fit$cluster, color=TRUE, shade=TRUE, \n   labels=3, lines=0)"
  },
  {
    "objectID": "class_notes/week_6.html#conditional-inference-tree-with-party",
    "href": "class_notes/week_6.html#conditional-inference-tree-with-party",
    "title": "Week 5: Regression and Clustering",
    "section": "Conditional Inference Tree with Party",
    "text": "Conditional Inference Tree with Party\nA conditional inference tree is basically a regression tree, and it tells you exactly how it picks apart the data in a pretty clear way.\nI’ve always thought that Weezer was a bit derivative, so we might look at how we can separate them other (much better) bands, like Pavement…\n\n###gets around the spotify API limitations of 100.\nsplit_into_chunks &lt;- function(track_ids, chunk_size = 100) {\n  split(track_ids, ceiling(seq_along(track_ids) / chunk_size))\n}\n\n\n\n\n###gets tracks from pavement and weezer albums, and all of their features.\npavement_albums &lt;- get_artist_albums('3inCNiUr4R6XQ3W43s9Aqi')\npavement_tracks &lt;- map_df(pavement_albums$id, get_album_tracks)\n\nweezer_albums &lt;- get_artist_albums('3jOstUTkEu2JkjvRdBA5Gu')\nweezer_tracks &lt;- map_df(weezer_albums$id, get_album_tracks)\n\n\n##splits into chunks and gets the audio data for weezer\nweezer_track_id_chunks &lt;- split_into_chunks(weezer_tracks$id)\nweezer_audio_features_list &lt;- lapply(weezer_track_id_chunks, get_track_audio_features)\n\n##splits into chunks and gets the audio data for pavement\npavement_track_id_chunks &lt;- split_into_chunks(pavement_tracks$id)\npavement_audio_features_list &lt;- lapply(pavement_track_id_chunks, get_track_audio_features)\n\n\n# Combine the list of data frames into a single data frame for both artists\nweezer_audio_features &lt;- do.call(rbind, weezer_audio_features_list)\npavement_audio_features &lt;- do.call(rbind, pavement_audio_features_list)\n\nweezer_data &lt;- left_join(weezer_tracks, weezer_audio_features, by = \"id\")\npavement_data &lt;- left_join(pavement_tracks, pavement_audio_features, by = \"id\")\npavement_data$artist &lt;- \"Pavement\"\nweezer_data$artist &lt;- \"Weezer\"\n\npavement &lt;- pavement_data |&gt; select(artist, danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)\nweezer &lt;- weezer_data |&gt; select(artist, danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)\n\npavement_weezer &lt;-rbind(pavement, weezer)\npavement_weezer &lt;- as.data.frame(pavement_weezer)\n\nHere’s a regression tree that tries to account for the variance between deciding whether a piece is from Pavement or Weezer.\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n# grow tree \nfit &lt;- rpart(as.factor(artist) ~ danceability + \n              valence + tempo + liveness,  data=pavement_weezer)\n\nprintcp(fit) # display the results \n\n\nClassification tree:\nrpart(formula = as.factor(artist) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n\nVariables actually used in tree construction:\n[1] danceability liveness     tempo        valence     \n\nRoot node error: 245/523 = 0.46845\n\nn= 523 \n\n        CP nsplit rel error  xerror     xstd\n1 0.297959      0   1.00000 1.00000 0.046579\n2 0.020408      1   0.70204 0.75510 0.044630\n3 0.018367      6   0.60000 0.76735 0.044790\n4 0.016327      8   0.56327 0.75102 0.044575\n5 0.012245     10   0.53061 0.75102 0.044575\n6 0.011224     12   0.50612 0.72653 0.044229\n7 0.010000     17   0.44082 0.73469 0.044347\n\nplotcp(fit) # visualize cross-validation results \n\n\n\nsummary(fit) # detailed summary of splits\n\nCall:\nrpart(formula = as.factor(artist) ~ danceability + valence + \n    tempo + liveness, data = pavement_weezer)\n  n= 523 \n\n          CP nsplit rel error    xerror       xstd\n1 0.29795918      0 1.0000000 1.0000000 0.04657882\n2 0.02040816      1 0.7020408 0.7551020 0.04463004\n3 0.01836735      6 0.6000000 0.7673469 0.04479034\n4 0.01632653      8 0.5632653 0.7510204 0.04457505\n5 0.01224490     10 0.5306122 0.7510204 0.04457505\n6 0.01122449     12 0.5061224 0.7265306 0.04422854\n7 0.01000000     17 0.4408163 0.7346939 0.04434722\n\nVariable importance\ndanceability        tempo      valence     liveness \n          37           26           21           15 \n\nNode number 1: 523 observations,    complexity param=0.2979592\n  predicted class=Pavement  expected loss=0.4684512  P(node) =1\n    class counts:   278   245\n   probabilities: 0.532 0.468 \n  left son=2 (276 obs) right son=3 (247 obs)\n  Primary splits:\n      danceability &lt; 0.4725   to the left,  improve=30.101410, (0 missing)\n      valence      &lt; 0.192    to the left,  improve= 6.770815, (0 missing)\n      liveness     &lt; 0.0561   to the right, improve= 5.761035, (0 missing)\n      tempo        &lt; 142.1145 to the right, improve= 5.082890, (0 missing)\n  Surrogate splits:\n      valence  &lt; 0.518    to the left,  agree=0.641, adj=0.239, (0 split)\n      tempo    &lt; 136.195  to the right, agree=0.606, adj=0.166, (0 split)\n      liveness &lt; 0.0561   to the right, agree=0.539, adj=0.024, (0 split)\n\nNode number 2: 276 observations,    complexity param=0.02040816\n  predicted class=Pavement  expected loss=0.307971  P(node) =0.5277247\n    class counts:   191    85\n   probabilities: 0.692 0.308 \n  left son=4 (135 obs) right son=5 (141 obs)\n  Primary splits:\n      tempo        &lt; 125.6825 to the left,  improve=4.586456, (0 missing)\n      valence      &lt; 0.462    to the right, improve=3.868215, (0 missing)\n      danceability &lt; 0.3985   to the left,  improve=3.317837, (0 missing)\n      liveness     &lt; 0.587    to the right, improve=2.857358, (0 missing)\n  Surrogate splits:\n      valence      &lt; 0.462    to the right, agree=0.591, adj=0.163, (0 split)\n      danceability &lt; 0.3295   to the right, agree=0.569, adj=0.119, (0 split)\n      liveness     &lt; 0.3165   to the right, agree=0.569, adj=0.119, (0 split)\n\nNode number 3: 247 observations,    complexity param=0.02040816\n  predicted class=Weezer    expected loss=0.3522267  P(node) =0.4722753\n    class counts:    87   160\n   probabilities: 0.352 0.648 \n  left son=6 (50 obs) right son=7 (197 obs)\n  Primary splits:\n      valence      &lt; 0.3135   to the left,  improve=4.420774, (0 missing)\n      tempo        &lt; 142.4715 to the right, improve=3.415253, (0 missing)\n      liveness     &lt; 0.3515   to the left,  improve=3.319289, (0 missing)\n      danceability &lt; 0.522    to the left,  improve=3.156231, (0 missing)\n\nNode number 4: 135 observations,    complexity param=0.01836735\n  predicted class=Pavement  expected loss=0.2148148  P(node) =0.2581262\n    class counts:   106    29\n   probabilities: 0.785 0.215 \n  left son=8 (96 obs) right son=9 (39 obs)\n  Primary splits:\n      tempo        &lt; 91.104   to the right, improve=4.189779, (0 missing)\n      liveness     &lt; 0.311    to the right, improve=3.744537, (0 missing)\n      valence      &lt; 0.5385   to the right, improve=1.336445, (0 missing)\n      danceability &lt; 0.21     to the right, improve=0.436410, (0 missing)\n  Surrogate splits:\n      danceability &lt; 0.2455   to the right, agree=0.756, adj=0.154, (0 split)\n      valence      &lt; 0.04065  to the right, agree=0.748, adj=0.128, (0 split)\n      liveness     &lt; 0.913    to the left,  agree=0.719, adj=0.026, (0 split)\n\nNode number 5: 141 observations,    complexity param=0.02040816\n  predicted class=Pavement  expected loss=0.3971631  P(node) =0.2695985\n    class counts:    85    56\n   probabilities: 0.603 0.397 \n  left son=10 (93 obs) right son=11 (48 obs)\n  Primary splits:\n      danceability &lt; 0.405    to the left,  improve=6.236817, (0 missing)\n      valence      &lt; 0.192    to the left,  improve=4.284825, (0 missing)\n      liveness     &lt; 0.1585   to the left,  improve=3.346640, (0 missing)\n      tempo        &lt; 173.12   to the right, improve=3.191540, (0 missing)\n  Surrogate splits:\n      tempo    &lt; 136.6285 to the right, agree=0.766, adj=0.313, (0 split)\n      liveness &lt; 0.1004   to the right, agree=0.688, adj=0.083, (0 split)\n      valence  &lt; 0.552    to the left,  agree=0.667, adj=0.021, (0 split)\n\nNode number 6: 50 observations,    complexity param=0.02040816\n  predicted class=Pavement  expected loss=0.46  P(node) =0.09560229\n    class counts:    27    23\n   probabilities: 0.540 0.460 \n  left son=12 (16 obs) right son=13 (34 obs)\n  Primary splits:\n      valence      &lt; 0.279    to the right, improve=3.494412, (0 missing)\n      liveness     &lt; 0.117    to the left,  improve=3.494412, (0 missing)\n      danceability &lt; 0.545    to the left,  improve=1.883478, (0 missing)\n      tempo        &lt; 122.7905 to the left,  improve=1.830476, (0 missing)\n  Surrogate splits:\n      tempo    &lt; 99.264   to the left,  agree=0.72, adj=0.125, (0 split)\n      liveness &lt; 0.483    to the right, agree=0.72, adj=0.125, (0 split)\n\nNode number 7: 197 observations,    complexity param=0.01632653\n  predicted class=Weezer    expected loss=0.3045685  P(node) =0.376673\n    class counts:    60   137\n   probabilities: 0.305 0.695 \n  left son=14 (18 obs) right son=15 (179 obs)\n  Primary splits:\n      tempo        &lt; 145.2105 to the right, improve=3.723037, (0 missing)\n      danceability &lt; 0.7155   to the right, improve=3.449435, (0 missing)\n      liveness     &lt; 0.3925   to the left,  improve=3.325543, (0 missing)\n      valence      &lt; 0.6665   to the right, improve=1.613554, (0 missing)\n\nNode number 8: 96 observations\n  predicted class=Pavement  expected loss=0.1354167  P(node) =0.1835564\n    class counts:    83    13\n   probabilities: 0.865 0.135 \n\nNode number 9: 39 observations,    complexity param=0.01836735\n  predicted class=Pavement  expected loss=0.4102564  P(node) =0.07456979\n    class counts:    23    16\n   probabilities: 0.590 0.410 \n  left son=18 (20 obs) right son=19 (19 obs)\n  Primary splits:\n      liveness     &lt; 0.164    to the right, improve=7.903374, (0 missing)\n      tempo        &lt; 77.903   to the left,  improve=3.938462, (0 missing)\n      valence      &lt; 0.489    to the left,  improve=1.538462, (0 missing)\n      danceability &lt; 0.4325   to the left,  improve=1.038462, (0 missing)\n  Surrogate splits:\n      danceability &lt; 0.4295   to the left,  agree=0.692, adj=0.368, (0 split)\n      tempo        &lt; 79.1165  to the left,  agree=0.667, adj=0.316, (0 split)\n      valence      &lt; 0.06715  to the left,  agree=0.615, adj=0.211, (0 split)\n\nNode number 10: 93 observations,    complexity param=0.01122449\n  predicted class=Pavement  expected loss=0.2903226  P(node) =0.1778203\n    class counts:    66    27\n   probabilities: 0.710 0.290 \n  left son=20 (28 obs) right son=21 (65 obs)\n  Primary splits:\n      liveness     &lt; 0.1325   to the left,  improve=3.839064, (0 missing)\n      valence      &lt; 0.467    to the right, improve=2.698114, (0 missing)\n      tempo        &lt; 137.6275 to the left,  improve=1.579362, (0 missing)\n      danceability &lt; 0.249    to the right, improve=1.275376, (0 missing)\n  Surrogate splits:\n      valence &lt; 0.192    to the left,  agree=0.796, adj=0.321, (0 split)\n      tempo   &lt; 171.5065 to the right, agree=0.742, adj=0.143, (0 split)\n\nNode number 11: 48 observations,    complexity param=0.0122449\n  predicted class=Weezer    expected loss=0.3958333  P(node) =0.0917782\n    class counts:    19    29\n   probabilities: 0.396 0.604 \n  left son=22 (36 obs) right son=23 (12 obs)\n  Primary splits:\n      liveness     &lt; 0.232    to the left,  improve=3.125000, (0 missing)\n      danceability &lt; 0.446    to the right, improve=2.858333, (0 missing)\n      tempo        &lt; 135.3625 to the right, improve=1.608768, (0 missing)\n      valence      &lt; 0.47     to the right, improve=0.939358, (0 missing)\n  Surrogate splits:\n      danceability &lt; 0.4235   to the right, agree=0.833, adj=0.333, (0 split)\n      tempo        &lt; 126.185  to the right, agree=0.812, adj=0.250, (0 split)\n\nNode number 12: 16 observations\n  predicted class=Pavement  expected loss=0.1875  P(node) =0.03059273\n    class counts:    13     3\n   probabilities: 0.813 0.187 \n\nNode number 13: 34 observations,    complexity param=0.02040816\n  predicted class=Weezer    expected loss=0.4117647  P(node) =0.06500956\n    class counts:    14    20\n   probabilities: 0.412 0.588 \n  left son=26 (9 obs) right son=27 (25 obs)\n  Primary splits:\n      liveness     &lt; 0.113    to the left,  improve=3.279477, (0 missing)\n      danceability &lt; 0.5015   to the left,  improve=2.409982, (0 missing)\n      valence      &lt; 0.252    to the left,  improve=1.719600, (0 missing)\n      tempo        &lt; 122.7905 to the left,  improve=1.719600, (0 missing)\n  Surrogate splits:\n      tempo &lt; 133.5595 to the right, agree=0.824, adj=0.333, (0 split)\n\nNode number 14: 18 observations\n  predicted class=Pavement  expected loss=0.3888889  P(node) =0.03441683\n    class counts:    11     7\n   probabilities: 0.611 0.389 \n\nNode number 15: 179 observations,    complexity param=0.01632653\n  predicted class=Weezer    expected loss=0.273743  P(node) =0.3422562\n    class counts:    49   130\n   probabilities: 0.274 0.726 \n  left son=30 (14 obs) right son=31 (165 obs)\n  Primary splits:\n      danceability &lt; 0.7155   to the right, improve=4.138552, (0 missing)\n      liveness     &lt; 0.07975  to the right, improve=2.633307, (0 missing)\n      tempo        &lt; 103.7565 to the right, improve=1.880171, (0 missing)\n      valence      &lt; 0.6665   to the right, improve=1.309757, (0 missing)\n\nNode number 18: 20 observations\n  predicted class=Pavement  expected loss=0.1  P(node) =0.03824092\n    class counts:    18     2\n   probabilities: 0.900 0.100 \n\nNode number 19: 19 observations\n  predicted class=Weezer    expected loss=0.2631579  P(node) =0.03632887\n    class counts:     5    14\n   probabilities: 0.263 0.737 \n\nNode number 20: 28 observations\n  predicted class=Pavement  expected loss=0.07142857  P(node) =0.05353728\n    class counts:    26     2\n   probabilities: 0.929 0.071 \n\nNode number 21: 65 observations,    complexity param=0.01122449\n  predicted class=Pavement  expected loss=0.3846154  P(node) =0.124283\n    class counts:    40    25\n   probabilities: 0.615 0.385 \n  left son=42 (14 obs) right son=43 (51 obs)\n  Primary splits:\n      valence      &lt; 0.467    to the right, improve=3.500323, (0 missing)\n      tempo        &lt; 135.526  to the left,  improve=3.496503, (0 missing)\n      liveness     &lt; 0.268    to the right, improve=1.782744, (0 missing)\n      danceability &lt; 0.2515   to the right, improve=1.454810, (0 missing)\n  Surrogate splits:\n      tempo &lt; 127.1735 to the left,  agree=0.815, adj=0.143, (0 split)\n\nNode number 22: 36 observations,    complexity param=0.0122449\n  predicted class=Pavement  expected loss=0.5  P(node) =0.06883365\n    class counts:    18    18\n   probabilities: 0.500 0.500 \n  left son=44 (12 obs) right son=45 (24 obs)\n  Primary splits:\n      danceability &lt; 0.4515   to the right, improve=2.250000, (0 missing)\n      tempo        &lt; 127.3205 to the left,  improve=2.216749, (0 missing)\n      liveness     &lt; 0.1185   to the right, improve=2.025000, (0 missing)\n      valence      &lt; 0.6305   to the left,  improve=1.285714, (0 missing)\n  Surrogate splits:\n      tempo   &lt; 187.986  to the right, agree=0.722, adj=0.167, (0 split)\n      valence &lt; 0.204    to the left,  agree=0.694, adj=0.083, (0 split)\n\nNode number 23: 12 observations\n  predicted class=Weezer    expected loss=0.08333333  P(node) =0.02294455\n    class counts:     1    11\n   probabilities: 0.083 0.917 \n\nNode number 26: 9 observations\n  predicted class=Pavement  expected loss=0.2222222  P(node) =0.01720841\n    class counts:     7     2\n   probabilities: 0.778 0.222 \n\nNode number 27: 25 observations\n  predicted class=Weezer    expected loss=0.28  P(node) =0.04780115\n    class counts:     7    18\n   probabilities: 0.280 0.720 \n\nNode number 30: 14 observations\n  predicted class=Pavement  expected loss=0.3571429  P(node) =0.02676864\n    class counts:     9     5\n   probabilities: 0.643 0.357 \n\nNode number 31: 165 observations\n  predicted class=Weezer    expected loss=0.2424242  P(node) =0.3154876\n    class counts:    40   125\n   probabilities: 0.242 0.758 \n\nNode number 42: 14 observations\n  predicted class=Pavement  expected loss=0.07142857  P(node) =0.02676864\n    class counts:    13     1\n   probabilities: 0.929 0.071 \n\nNode number 43: 51 observations,    complexity param=0.01122449\n  predicted class=Pavement  expected loss=0.4705882  P(node) =0.09751434\n    class counts:    27    24\n   probabilities: 0.529 0.471 \n  left son=86 (7 obs) right son=87 (44 obs)\n  Primary splits:\n      tempo        &lt; 135.526  to the left,  improve=3.593583, (0 missing)\n      liveness     &lt; 0.268    to the right, improve=2.440336, (0 missing)\n      danceability &lt; 0.3595   to the right, improve=1.767320, (0 missing)\n      valence      &lt; 0.4395   to the left,  improve=1.481532, (0 missing)\n  Surrogate splits:\n      valence      &lt; 0.10195  to the left,  agree=0.922, adj=0.429, (0 split)\n      danceability &lt; 0.401    to the right, agree=0.902, adj=0.286, (0 split)\n\nNode number 44: 12 observations\n  predicted class=Pavement  expected loss=0.25  P(node) =0.02294455\n    class counts:     9     3\n   probabilities: 0.750 0.250 \n\nNode number 45: 24 observations\n  predicted class=Weezer    expected loss=0.375  P(node) =0.0458891\n    class counts:     9    15\n   probabilities: 0.375 0.625 \n\nNode number 86: 7 observations\n  predicted class=Pavement  expected loss=0  P(node) =0.01338432\n    class counts:     7     0\n   probabilities: 1.000 0.000 \n\nNode number 87: 44 observations,    complexity param=0.01122449\n  predicted class=Weezer    expected loss=0.4545455  P(node) =0.08413002\n    class counts:    20    24\n   probabilities: 0.455 0.545 \n  left son=174 (33 obs) right son=175 (11 obs)\n  Primary splits:\n      tempo        &lt; 146.2065 to the right, improve=6.0606060, (0 missing)\n      valence      &lt; 0.22     to the right, improve=1.6174100, (0 missing)\n      liveness     &lt; 0.268    to the right, improve=0.9630094, (0 missing)\n      danceability &lt; 0.2515   to the right, improve=0.6687565, (0 missing)\n  Surrogate splits:\n      danceability &lt; 0.3915   to the left,  agree=0.818, adj=0.273, (0 split)\n\nNode number 174: 33 observations,    complexity param=0.01122449\n  predicted class=Pavement  expected loss=0.3939394  P(node) =0.06309751\n    class counts:    20    13\n   probabilities: 0.606 0.394 \n  left son=348 (26 obs) right son=349 (7 obs)\n  Primary splits:\n      valence      &lt; 0.4395   to the left,  improve=3.8125210, (0 missing)\n      tempo        &lt; 156.6715 to the left,  improve=2.4729600, (0 missing)\n      danceability &lt; 0.221    to the right, improve=1.8235100, (0 missing)\n      liveness     &lt; 0.4165   to the left,  improve=0.5597736, (0 missing)\n\nNode number 175: 11 observations\n  predicted class=Weezer    expected loss=0  P(node) =0.0210325\n    class counts:     0    11\n   probabilities: 0.000 1.000 \n\nNode number 348: 26 observations\n  predicted class=Pavement  expected loss=0.2692308  P(node) =0.04971319\n    class counts:    19     7\n   probabilities: 0.731 0.269 \n\nNode number 349: 7 observations\n  predicted class=Weezer    expected loss=0.1428571  P(node) =0.01338432\n    class counts:     1     6\n   probabilities: 0.143 0.857 \n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Pavement/Weezer\")"
  },
  {
    "objectID": "class_notes/week_6.html#christmas-or-not",
    "href": "class_notes/week_6.html#christmas-or-not",
    "title": "Week 5: Regression and Clustering",
    "section": "Christmas or Not?",
    "text": "Christmas or Not?\n\nchristmas &lt;- get_playlist_audio_features(\"\", \"5OP7itTh52BMfZS1DJrdlv\")\nchristmas$christmas &lt;- \"yes\"\n\nnot &lt;- get_playlist_audio_features(\"\", \"6i2Qd6OpeRBAzxfscNXeWp\")\nnot$christmas &lt;- \"no\"\nchristmas_not &lt;-rbind(christmas, not)\n\nfit &lt;- rpart(as.factor(christmas) ~ danceability + valence + tempo + liveness + tempo + mode, data=christmas_not)\n\n# plot tree \nrpart.plot(fit, uniform=TRUE, \n   main=\"Regression Tree for Christmas/Not\")\n\n\n\n\n\ntable(not$mode)\n\n\n 0  1 \n27 85"
  }
]