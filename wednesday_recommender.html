{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Week 9: Recommender Systems\"\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan for the Week \n",
        "\n",
        "- Talking about Recommender Systems\n",
        "- What kind of data would you need for your project? \n",
        "- Monday: Basic Recommender Code \n",
        "- Wednesday: Including Spotify Features\n",
        "\n",
        "\n",
        "# Monday\n",
        "\n",
        "The code below generates fake ratings for each user for a bunch of songs: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define users and songs\n",
        "users = ['Alex', 'Stephanie', 'Hannah', 'Kelvin', 'Khoi', 'Leo', 'Ally', 'Jordan', 'Sid', 'Jacob']\n",
        "songs = [\n",
        "    'Respect', 'In The Mood', 'White Christmas', 'Johnny B. Goode', 'Over The Rainbow',\n",
        "    'Thriller', 'Imagine', 'Billie Jean', 'Hotel California', 'Stairway to Heaven', 'Shake it Off', \n",
        "    'As it Was', 'Truth Hurts', 'All I Want for Christmas', 'Fade to Black', 'Enter Sandman', 'Smells Like Teen Spirit',\n",
        "    'Paranoid Android', 'Sgt. Pepper', 'Help', 'Good Vibrations', 'Happy', '2 Legit 2 Quit', 'Happy Birthday', 'Happy', \n",
        "    'The Theme from the 90s TV Show E.R', 'Elephants Playing Gongs', 'Raindrops Keep Falling on My Head', 'Beethoven 9',\n",
        "    'Dolphins Singing', 'The NU Fight Song', 'Bach?', 'Do you really want to hurt me?', 'Schools Out', 'Jesses Girl',\n",
        "    'Dan Shanahan Yelling', 'Dan Shanahan Rocking on the Guitar', 'Sounds of a Campfire'\n",
        "\n",
        "]\n",
        "\n",
        "# Generate random ratings\n",
        "data = []\n",
        "for i in range(2000):\n",
        "    user = random.choice(users)\n",
        "    song = random.choice(songs)\n",
        "    rating = random.randint(1, 5)\n",
        "    data.append({'user_id': user, 'track_title': song, 'ratings': rating})\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df = pd.DataFrame(data)\n",
        "# if you want to save it as a csv:\n",
        "# df.to_csv('spotify_data.csv', index=False)\n",
        "print(df.head(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It ends up looking something like this:\n",
        "\n",
        "```{txt}\n",
        "      user_id       track_title  ratings\n",
        "0         Leo       Schools Out        2\n",
        "1      Jordan     Enter Sandman        4\n",
        "2       Jacob  Over The Rainbow        1\n",
        "3      Jordan  Paranoid Android        4\n",
        "4      Hannah     Fade to Black        1\n",
        "..        ...               ...      ...\n",
        "95        Leo       Jesses Girl        4\n",
        "96      Jacob             Happy        4\n",
        "97  Stephanie     Fade to Black        4\n",
        "98     Kelvin       Truth Hurts        2\n",
        "99        Leo              Help        1\n",
        "```\n",
        "\n",
        "### Beginning Recommender Code with Surprise\n",
        "\n",
        "First, we will begin by loading all of the libraries. Here, I'm going to be using pandas, and a number of modules from the Surprise toolkit. If you don't already have it installed, typing `pip intall surprise` into your Python environment will install it for you.\n",
        "\n",
        "Once the libraries are loaded, you can import the dataframe `df` that we created above, and load it into surprise's `Reader` object. Then, we can do an 80/20 training and testing split quite easily with the built-in train_test_split function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic, KNNWithMeans\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Create a Surprise Reader object\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load the data into the Surprise Dataset format\n",
        "data = Dataset.load_from_df(df[['user_id', 'track_title', 'ratings']], reader)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainset, testset = train_test_split(data, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it's time to, in effect, fill in the blank spots in our matrix. Surprise has a number of algorithms built in. Singular Value Decomposition (SVD) is probably the most commonly-used one, but there are many KNNs bundled, as well. Here I've used the KNNBasic. I then use that algorithm to train on the training dataset, and then predict with the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use SVD algorithm\n",
        "# algo = SVD()\n",
        "algo = KNNBasic()\n",
        "# algo = KNNWithMeans()\n",
        "\n",
        "# Train the algorithm on the trainset\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make predictions on the testset\n",
        "predictions = algo.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I could also look at a content based system with a different form of similarity, here using a geometric approach, if I wanted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sim_options = {\n",
        "    \"name\": \"cosine\",\n",
        "    \"user_based\": False,  # compute  similarities between items, not users!!!\n",
        "}\n",
        "algo = KNNBasic(sim_options=sim_options)\n",
        "# algo = KNNWithMeans()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some functions for getting the top recommendations for each user. Note that I loop over the user id, item id, and rating, as well as the estimate. I then grab the top 10 from these and sort the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get top N recommendations for a user\n",
        "def get_top_n_recommendations(predictions, n=10):\n",
        "    top_n = {}\n",
        "    for user_id, item_id, true_rating, est, i in predictions:\n",
        "        if user_id not in top_n:\n",
        "            top_n[user_id] = []\n",
        "        top_n[user_id].append((item_id, est))\n",
        "    \n",
        "    # Sort the predictions for each user and retrieve the top ones\n",
        "    for user_id, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[user_id] = user_ratings[:n]\n",
        "    \n",
        "    return top_n\n",
        "\n",
        "# Get top 5 recommendations for each user\n",
        "top_n = get_top_n_recommendations(predictions, n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is just a printing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print the top 5 recommendations\n",
        "for i, (user, recommendations) in enumerate(list(top_n.items())):\n",
        "    print(f\"\\nTop track recommendations for user {user}:\")\n",
        "    for j, (track_id, estimated_rating) in enumerate(recommendations, 10):\n",
        "        print(f\"Track Title: {track_id}, Estimated Rating: {estimated_rating:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And this gets the following results:\n",
        "\n",
        "```{txt}\n",
        "Computing the msd similarity matrix...\n",
        "Done computing similarity matrix.\n",
        "\n",
        "Top track recommendations for user Khoi:\n",
        "Track Title: Schools Out, Estimated Rating: 3.38\n",
        "Track Title: Help, Estimated Rating: 3.29\n",
        "Track Title: Beethoven 9, Estimated Rating: 3.18\n",
        "Track Title: Over The Rainbow, Estimated Rating: 3.16\n",
        "Track Title: Stairway to Heaven, Estimated Rating: 3.14\n",
        "\n",
        "Top track recommendations for user Hannah:\n",
        "Track Title: Good Vibrations, Estimated Rating: 3.39\n",
        "Track Title: Good Vibrations, Estimated Rating: 3.39\n",
        "Track Title: Good Vibrations, Estimated Rating: 3.39\n",
        "Track Title: Imagine, Estimated Rating: 3.32\n",
        "Track Title: Imagine, Estimated Rating: 3.32\n",
        "\n",
        "etc.\n",
        "```\n",
        "\n",
        "The standard way for evaluating the accuracy of the algorithm is the RMSE (root mean square error). Again, Surprise has a built-in method for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#rmse = accuracy.rmse(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we end up with:\n",
        "\n",
        "```{txt}\n",
        "RMSE: 1.4986\n",
        "```\n",
        "\n",
        "This isn't great. I think for movies, I've seen people shoot for .8; for other things it's even better than that (lower is better).\n",
        "\n",
        "### Visualizing the Similarity\n",
        "\n",
        "We can look at the similarity of items with a bit of help from Seaborn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# # Extract the similarity matrix\n",
        "# sim_matrix = algo.sim\n",
        "# \n",
        "# # Convert to a pandas DataFrame\n",
        "# df_sim = pd.DataFrame(sim_matrix)\n",
        "\n",
        "# Create a heatmap\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# sns.heatmap(df_sim, cmap='YlGnBu')\n",
        "# plt.title('Item Similarity Matrix Heatmap')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will give us this:\n",
        "![Item Similarity Matrix](../images/item-similarity-matrix.png)\n",
        "\n",
        "\n",
        "## The Cold Start Problem\n",
        "\n",
        "This was all random data. How will you get data for your project? What will it look like for your model to be accurate?\n",
        "\n",
        "\n",
        "## Two Spotify Discover Playlists\n",
        "\n",
        "What if I take my own discover playlist, and find someone else's and see what it might recommend?\n",
        "\n",
        "We can start by getting data from playlists. Put in your own Spotify data here. I was having some cache issues with my client id, so I'm using the `MemoryCacheHandler` function from Spotipy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "from surprise import Dataset, Reader, KNNBasic\n",
        "import pandas as pd\n",
        "from spotipy.cache_handler import MemoryCacheHandler\n",
        "\n",
        "\n",
        "client_credentials_manager = SpotifyClientCredentials(\n",
        "    client_id = 'YOUR OWN CLIENT ID',\n",
        "    client_secret = 'YOUR OWN CLIENT SECRET',\n",
        "    cache_handler=MemoryCacheHandler()\n",
        ")\n",
        "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a function that grabs the audio data from Spotify, using spotipy and pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get playlist tracks and audio features\n",
        "def get_playlist_data(playlist_id):\n",
        "    results = sp.playlist_tracks(playlist_id)\n",
        "    tracks = results['items']\n",
        "    \n",
        "    data = []\n",
        "    for track in tracks:\n",
        "        track_id = track['track']['id']\n",
        "        audio_features = sp.audio_features(track_id)[0]\n",
        "        \n",
        "        data.append({\n",
        "            'track_id': track_id,\n",
        "            'acousticness': audio_features['acousticness'],\n",
        "            'danceability': audio_features['danceability'],\n",
        "            'energy': audio_features['energy'],\n",
        "            'instrumentalness': audio_features['instrumentalness'],\n",
        "            'liveness': audio_features['liveness']\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can add all of the Spotify URIs for the playlist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get data for both playlists\n",
        "playlist1_id = '37i9dQZEVXcIDdjTZ1FKnJ'\n",
        "playlist2_id = '2VYB4MjPYJx3ZOkrjyUvsx'\n",
        "\n",
        "df1 = get_playlist_data(playlist1_id)\n",
        "df2 = get_playlist_data(playlist2_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will read it into Surprise with the Reader function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assign user IDs (1 for playlist1, 2 for playlist2)\n",
        "df1['user_id'] = 'Dannah Shanalee'\n",
        "df2['user_id'] = 'Hannah Ashlee'\n",
        "\n",
        "# Combine dataframes\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Prepare data for Surprise\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "data = Dataset.load_from_df(combined_df[['user_id', 'track_id', 'acousticness']], reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have a function that gets the track information (name, artist, and id) to print it later (it was only printing URIs), and then recommend 5 tracks from the other playlist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_track_info(track_id):\n",
        "    try:\n",
        "        track_info = sp.track(track_id)\n",
        "        return {\n",
        "            'name': track_info['name'],\n",
        "            'artist': track_info['artists'][0]['name'],\n",
        "            'id': track_id\n",
        "        }\n",
        "    except:\n",
        "        return {'name': 'Unknown', 'artist': 'Unknown', 'id': track_id}\n",
        "\n",
        "def get_recommendations(track_id, model, n=5):\n",
        "    inner_id = model.trainset.to_inner_iid(track_id)\n",
        "    neighbors = model.get_neighbors(inner_id, k=n)\n",
        "    raw_neighbors = [model.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n",
        "    return [get_track_info(track_id) for track_id in raw_neighbors]\n",
        "\n",
        "# Example: Get recommendations for a track\n",
        "sample_track = combined_df['track_id'].iloc[0]\n",
        "sample_track_info = get_track_info(sample_track)\n",
        "recommendations = get_recommendations(sample_track, algo)\n",
        "\n",
        "print(f\"Recommendations for '{sample_track_info['name']}' by {sample_track_info['artist']}:\")\n",
        "for rec in recommendations:\n",
        "    print(f\"- '{rec['name']}' by {rec['artist']} (ID: {rec['id']})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we get five new recommendations:\n",
        "\n",
        "```{txt}\n",
        "\n",
        "Recommendations for 'Lily Pad on Your Doorstep' by Don't Stop or We'll Die:\n",
        "- 'Amazing Glow' by Pernice Brothers (ID: 6ViAxolMqWD1d6JrqgPZKc)\n",
        "- 'Rang Tang Ring Toon' by Mountain Man (ID: 2HZRjBrPeo3HwyZVUZxK62)\n",
        "- 'Amsterdam' by Guster (ID: 3fv9cBtpMOaFaIAO4uVRBV)\n",
        "- 'Bernadette' by Elle Cordova (ID: 4NFhb4AOPBulDFgxyoXaLH)\n",
        "- 'Tire Swing' by Kimya Dawson (ID: 0vbhRDi46TDNHkhKbZa81B)\n",
        "```\n",
        "\n",
        "After working through this example, in class, we realized that it wasn't really doing what we thought it was doing. We thought it was picking the top tracks from the other playlist, but in fact it was taking a *single track* and picking five similar tracks from *that same playlist*. \n",
        "\n",
        "So for Wednesday we want to do a few things:\n",
        "\n",
        "1. Look at two playlists, and **use the entire set of songs** to predict a single new song from playlist 1 for the user of playlist 2.\n",
        "2. We want to compare a few different models and see what this looks like, comparing the accuracy as needed.\n",
        "\n",
        "# Wednesday\n",
        "\n",
        "So let's start from scratch.\n",
        "\n",
        "## Importing the Libraries and the Data\n",
        "\n",
        "First, I'm going to import the spotipy, pandas, and surprise. I'm also going to import specific authentication and cache things with spotipy, and some models with Surprise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spotipy\n",
        "import os\n",
        "from surprise import Dataset, Reader, KNNBasic, SVD\n",
        "import pandas as pd\n",
        "from spotipy.cache_handler import MemoryCacheHandler\n",
        "from surprise.model_selection import cross_validate\n",
        "from spotipy.oauth2 import SpotifyClientCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've stored my id stuff locally so you can't add things to my playlists..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "\n",
        "# Access environment variables\n",
        "client_id = os.getenv('SPOTIFY_CLIENT_ID')\n",
        "client_secret = os.getenv('SPOTIFY_CLIENT_SECRET')\n",
        "\n",
        "# Set up Spotify client credentials\n",
        "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=client_id, client_secret=client_secret))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "print(os.getenv('SPOTIFY_CLIENT_ID'))\n",
        "print(os.getenv('SPOTIFY_CLIENT_SECRET'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that those are loaded, let's grab the data from two playlists from Spotify. \n",
        "\n",
        "We will start with a function that takes the playlist uri as an argument, grabs the audio features from the tracks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get playlist tracks and audio features\n",
        "def get_playlist_data(playlist_id):\n",
        "    results = sp.playlist_tracks(playlist_id)\n",
        "    tracks = results['items']\n",
        "    \n",
        "    data = []\n",
        "    for track in tracks:\n",
        "        track_id = track['track']['id']\n",
        "        audio_features = sp.audio_features(track_id)[0]\n",
        "        \n",
        "        data.append({\n",
        "            'track_id': track_id,\n",
        "            'acousticness': audio_features['acousticness'],\n",
        "            'danceability': audio_features['danceability'],\n",
        "            'energy': audio_features['energy'],\n",
        "            'instrumentalness': audio_features['instrumentalness'],\n",
        "            'liveness': audio_features['liveness'],\n",
        "            'mode': audio_features['mode'],\n",
        "            'tempo': audio_features['tempo'],\n",
        "\n",
        "\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this function and grab the information from the playlist with the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get data for both playlists\n",
        "playlist1_id = '37i9dQZEVXcIDdjTZ1FKnJ'\n",
        "playlist2_id = '2VYB4MjPYJx3ZOkrjyUvsx'\n",
        "\n",
        "df1 = get_playlist_data(playlist1_id)\n",
        "df2 = get_playlist_data(playlist2_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what kind of data we have from this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how it gets the track URI, and then all of the variables we asked for. All of these are continuous, with the exception of mode, which is a binary (0 or 1).\n",
        "\n",
        "We can add our user data here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assign user IDs (1 for playlist1, 2 for playlist2)\n",
        "df1['user_id'] = 'Dannah Shanalee'\n",
        "df2['user_id'] = 'Hannah Ashlee'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can see the data has specific user names (here only the first five rows)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can combine the two dataframes into one here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "combined_df['user_id'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see (using the `value_counts` function in `pandas`), that there are 100 songs in Hannah Ashlee's list, and 30 in mine. Not ideal, but we will try to see where it takes us.\n",
        "\n",
        "Now we will load the data into the surprise toolkkit, using the `Reader` function. The rating_scale option gives it a range of ratings. Here it's just 0 or 1, as it's if it's present in a playlist or not, but it could also be from 1 to 5 (if it's reviews) or something like that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reader = Reader(rating_scale=(0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `load_from_df` function in `surprise` is meant to accept three things: the user id, the item id, and a rating. Here, we don't really have a rating, which again isn't ideal. So here I'm just adding in a third thing (acousticness)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = Dataset.load_from_df(combined_df[['user_id', 'track_id', 'acousticness']], reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an Item-Item Recommender System\n",
        "\n",
        "Here I'm going to look at the `sim_options` argument for surprise, which is short for \"similarity options\". \n",
        "\n",
        "There are a few options to try for the name argument:\n",
        "\n",
        "1. `cosine` give us [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "2. `msd` gives use the [mean squared difference similarity](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
        "3. `pearson` gives us the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
        "4. `pearson_baseline` gives us the pearson correlation coefficient with a baseline estimate for ratings, which is meant to account for the overall average rating, and any user or item bias. \n",
        "\n",
        "For now, we can just use `pearson`, but it's worth comparing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## I'm going to give options for the model here, including specific model, and whether it's item based or user based.\n",
        "sim_options = {\n",
        "    'name': 'msd',\n",
        "    'user_based': False  # Item-based similarity\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is my code for implementing a K-nearest neighbor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dans_algorithm = KNNBasic(sim_options=sim_options)\n",
        "trainset = data.build_full_trainset()\n",
        "dans_algorithm.fit(trainset)\n",
        "sim_matrix = dans_algorithm.compute_similarities()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gives us a similarity matrix between items. Here I'll convert it to a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sim_matrix\n",
        "# Convert to a pandas DataFrame\n",
        "df_sim = pd.DataFrame(sim_matrix)\n",
        "df_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is telling me pairwise similarities using msd between the songs in the columns and the songs in the rows.\n",
        "\n",
        "Now that we have similarities, and compare between lists.\n",
        "\n",
        "\n",
        "## Using Data from One List to Recommend to Another\n",
        "\n",
        "Let's start with a function to grab the track info based on the URI. This isn't necessary, but if we want to be able to interpret anything past the URI, we should do this. \n",
        "\n",
        "NOTE: I kept getting errors because of Unknown IDs, it would choke the Spotify API up here. the `try` and `except` are meant to get around that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_track_info(track_id):\n",
        "    try:\n",
        "        track_info = sp.track(track_id)\n",
        "        return {\n",
        "            'name': track_info['name'],\n",
        "            'artist': track_info['artists'][0]['name'],\n",
        "            'id': track_id\n",
        "        }\n",
        "    except:\n",
        "        return {'name': 'Unknown', 'artist': 'Unknown', 'id': track_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can take those original track URIs and write a function to get the recommendations. Here, we get the \"inner id\", which is basically an internal representation used by `surprise`. Then we get the neighbors. \n",
        "\n",
        "A couple of other things:\n",
        "\n",
        "- k is referring to the amount of neighbors; here I've set it to 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_recommendations(track_id, model, n=10):\n",
        "    inner_id = model.trainset.to_inner_iid(track_id)\n",
        "    neighbors = model.get_neighbors(inner_id, k=n)\n",
        "    raw_neighbors = [model.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n",
        "    return [get_track_info(track_id) for track_id in raw_neighbors]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is code to get the recommendations *for myself* based on this other playlist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get recommendations for the user from playlist 2\n",
        "user_id = 'Dannah Shanalee'\n",
        "playlist2_tracks = df2['track_id'].tolist()\n",
        "recommendations = []\n",
        "for track_id in playlist2_tracks:\n",
        "    recs = get_recommendations(track_id, dans_algorithm, n=10)\n",
        "    recommendations.extend(recs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the top 10 recommendations in a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(recommendations).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it might be worth using the `cross_validate` function to see how well this did.\n",
        "\n",
        "In the code below, I use a 5-fold cross validation. Just as a reminder, this does the following steps:\n",
        "\n",
        "1. Partitions the data into 5 \"folds\"\n",
        "2. It then trains the data on 4 folds, and tests it on the other.\n",
        "\n",
        "Then, I look at both \"RMSE\" (Root Mean Square Error) and \"MAE\" (Mean Absolute Error). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = cross_validate(dans_algorithm, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And as a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd_results = pd.DataFrame(results)\n",
        "pd_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I had to look up what \"good\" RMSE numbers might be for this, and it seems that with a binary rating (as we have here) or lower is considered \"good\", so I think we've done a decent job here.\n",
        "\n",
        "# Some Future Steps\n",
        "\n",
        "1. Here we don't have ratings, which would have actually made this much easier. How might you get ratings?\n",
        "2. Here we are just using audio data, but metadata would really help. We could look at things like genre or artist between playlists, and pre-process the data with a one-hot encoding or something like that.\n",
        "\n",
        "This unworkable code might be one way to do it?:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# \n",
        "# # Example DataFrame\n",
        "# data = {\n",
        "#     'user_id': [1, 1, 2, 2, 3, 3],\n",
        "#     'track_id': [101, 102, 101, 103, 102, 104],\n",
        "#     'genres': [['rock', 'pop'], ['pop'], ['rock'], ['jazz'], ['pop', 'jazz'], ['rock', 'jazz']]\n",
        "# }\n",
        "# df = pd.DataFrame(data)\n",
        "# \n",
        "# # One-hot encode the genres\n",
        "# mlb = MultiLabelBinarizer()\n",
        "# genre_encoded = mlb.fit_transform(df['genres'])\n",
        "# genre_df = pd.DataFrame(genre_encoded, columns=mlb.classes_)\n",
        "# \n",
        "# # Combine with user_id and track_id\n",
        "# combined_df = pd.concat([df[['user_id', 'track_id']], genre_df], axis=1)\n",
        "# \n",
        "# # Display the combined DataFrame\n",
        "# print(combined_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}